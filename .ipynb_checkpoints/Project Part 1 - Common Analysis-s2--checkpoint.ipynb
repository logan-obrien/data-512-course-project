{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678b28cb",
   "metadata": {},
   "source": [
    "# Project Part 1 - Common Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f6063",
   "metadata": {},
   "source": [
    "**Author: Logan O'Brien** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39413436",
   "metadata": {},
   "source": [
    "## Step 1 Continued: Create the Fire Smoke Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2fabc",
   "metadata": {},
   "source": [
    "Once again we begin by importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3030628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from pyproj import Transformer, Geod\n",
    "\n",
    "#importing [4]\n",
    "from wildfire.Reader import Reader as WFReader\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e583c81d",
   "metadata": {},
   "source": [
    "**Note, I heavily rely upon Dr. McDonald's \"wildfire_geo_proximity_example.ipynb\" to learn how the wildfire data is structured as well as how to work with it [3]. In the subsequent steps, I will call out when I use code from that notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc131ae",
   "metadata": {},
   "source": [
    "Now, at this point we have already filtered the wildfire data to the fires we care about in another notebook. So let's load that filtered data and continue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45280a08",
   "metadata": {},
   "source": [
    "We begin by loading the data, where I employ modified code from two cells in [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4c22ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to open './data/relevant_fires.json' with wildfire.Reader() object\n"
     ]
    }
   ],
   "source": [
    "# I modified this file path to make it point to where I stored the file\n",
    "# DATA_FILENAME = './wildfire/Wildfire_short_sample.json'\n",
    "DATA_FILENAME = './data/relevant_fires.json'\n",
    "\n",
    "#\n",
    "#    This bit of code opens a new wildfire reader\n",
    "#\n",
    "print(f\"Attempting to open '{DATA_FILENAME}' with wildfire.Reader() object\")\n",
    "wfreader = WFReader(DATA_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c855b4",
   "metadata": {},
   "source": [
    "Now, let's use the Professor's Reader program in his wildfire module to load the data [4]. I employ a cell from [3] and modify some comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9250bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 features\n",
      "Loaded 200 features\n",
      "Loaded 300 features\n",
      "Loaded 400 features\n",
      "Loaded 500 features\n",
      "Loaded 600 features\n",
      "Loaded 700 features\n",
      "Loaded 800 features\n",
      "Loaded 900 features\n",
      "Loaded 1000 features\n",
      "Loaded 1100 features\n",
      "Loaded 1200 features\n",
      "Loaded 1300 features\n",
      "Loaded 1400 features\n",
      "Loaded 1500 features\n",
      "Loaded 1600 features\n",
      "Loaded 1700 features\n",
      "Loaded 1800 features\n",
      "Loaded 1900 features\n",
      "Loaded 2000 features\n",
      "Loaded 2100 features\n",
      "Loaded 2200 features\n",
      "Loaded 2300 features\n",
      "Loaded 2400 features\n",
      "Loaded 2500 features\n",
      "Loaded 2600 features\n",
      "Loaded 2700 features\n",
      "Loaded 2800 features\n",
      "Loaded 2900 features\n",
      "Loaded 3000 features\n",
      "Loaded 3100 features\n",
      "Loaded 3200 features\n",
      "Loaded 3300 features\n",
      "Loaded 3400 features\n",
      "Loaded 3500 features\n",
      "Loaded 3600 features\n",
      "Loaded 3700 features\n",
      "Loaded 3800 features\n",
      "Loaded 3900 features\n",
      "Loaded 4000 features\n",
      "Loaded 4100 features\n",
      "Loaded 4200 features\n",
      "Loaded 4300 features\n",
      "Loaded 4400 features\n",
      "Loaded 4500 features\n",
      "Loaded 4600 features\n",
      "Loaded 4700 features\n",
      "Loaded 4800 features\n",
      "Loaded 4900 features\n",
      "Loaded 5000 features\n",
      "Loaded 5100 features\n",
      "Loaded 5200 features\n",
      "Loaded 5300 features\n",
      "Loaded 5400 features\n",
      "Loaded 5500 features\n",
      "Loaded 5600 features\n",
      "Loaded 5700 features\n",
      "Loaded 5800 features\n",
      "Loaded 5900 features\n",
      "Loaded 6000 features\n",
      "Loaded 6100 features\n",
      "Loaded 6200 features\n",
      "Loaded 6300 features\n",
      "Loaded 6400 features\n",
      "Loaded 6500 features\n",
      "Loaded 6600 features\n",
      "Loaded 6700 features\n",
      "Loaded 6800 features\n",
      "Loaded 6900 features\n",
      "Loaded 7000 features\n",
      "Loaded 7100 features\n",
      "Loaded 7200 features\n",
      "Loaded 7300 features\n",
      "Loaded 7400 features\n",
      "Loaded 7500 features\n",
      "Loaded 7600 features\n",
      "Loaded 7700 features\n",
      "Loaded 7800 features\n",
      "Loaded 7900 features\n",
      "Loaded 8000 features\n",
      "Loaded 8100 features\n",
      "Loaded 8200 features\n",
      "Loaded 8300 features\n",
      "Loaded 8400 features\n",
      "Loaded 8500 features\n",
      "Loaded 8600 features\n",
      "Loaded 8700 features\n",
      "Loaded 8800 features\n",
      "Loaded 8900 features\n",
      "Loaded 9000 features\n",
      "Loaded 9100 features\n",
      "Loaded 9200 features\n",
      "Loaded 9300 features\n",
      "Loaded 9400 features\n",
      "Loaded 9500 features\n",
      "Loaded 9600 features\n",
      "Loaded 9700 features\n",
      "Loaded 9800 features\n",
      "Loaded 9900 features\n",
      "Loaded 10000 features\n",
      "Loaded 10100 features\n",
      "Loaded 10200 features\n",
      "Loaded 10300 features\n",
      "Loaded 10400 features\n",
      "Loaded 10500 features\n",
      "Loaded 10600 features\n",
      "Loaded 10700 features\n",
      "Loaded 10800 features\n",
      "Loaded 10900 features\n",
      "Loaded 11000 features\n",
      "Loaded 11100 features\n",
      "Loaded 11200 features\n",
      "Loaded 11300 features\n",
      "Loaded 11400 features\n",
      "Loaded 11500 features\n",
      "Loaded 11600 features\n",
      "Loaded 11700 features\n",
      "Loaded 11800 features\n",
      "Loaded 11900 features\n",
      "Loaded 12000 features\n",
      "Loaded 12100 features\n",
      "Loaded 12200 features\n",
      "Loaded 12300 features\n",
      "Loaded 12400 features\n",
      "Loaded 12500 features\n",
      "Loaded 12600 features\n",
      "Loaded 12700 features\n",
      "Loaded 12800 features\n",
      "Loaded 12900 features\n",
      "Loaded 13000 features\n",
      "Loaded 13100 features\n",
      "Loaded 13200 features\n",
      "Loaded 13300 features\n",
      "Loaded 13400 features\n",
      "Loaded 13500 features\n",
      "Loaded 13600 features\n",
      "Loaded 13700 features\n",
      "Loaded 13800 features\n",
      "Loaded 13900 features\n",
      "Loaded 14000 features\n",
      "Loaded 14100 features\n",
      "Loaded 14200 features\n",
      "Loaded 14300 features\n",
      "Loaded 14400 features\n",
      "Loaded 14500 features\n",
      "Loaded 14600 features\n",
      "Loaded 14700 features\n",
      "Loaded 14800 features\n",
      "Loaded 14900 features\n",
      "Loaded 15000 features\n",
      "Loaded 15100 features\n",
      "Loaded 15200 features\n",
      "Loaded 15300 features\n",
      "Loaded 15400 features\n",
      "Loaded 15500 features\n",
      "Loaded 15600 features\n",
      "Loaded 15700 features\n",
      "Loaded 15800 features\n",
      "Loaded 15900 features\n",
      "Loaded 16000 features\n",
      "Loaded 16100 features\n",
      "Loaded 16200 features\n",
      "Loaded 16300 features\n",
      "Loaded 16400 features\n",
      "Loaded 16500 features\n",
      "Loaded 16600 features\n",
      "Loaded 16700 features\n",
      "Loaded 16800 features\n",
      "Loaded 16900 features\n",
      "Loaded 17000 features\n",
      "Loaded 17100 features\n",
      "Loaded 17200 features\n",
      "Loaded 17300 features\n",
      "Loaded 17400 features\n",
      "Loaded 17500 features\n",
      "Loaded 17600 features\n",
      "Loaded 17700 features\n",
      "Loaded 17800 features\n",
      "Loaded 17900 features\n",
      "Loaded 18000 features\n",
      "Loaded 18100 features\n",
      "Loaded 18200 features\n",
      "Loaded 18300 features\n",
      "Loaded 18400 features\n",
      "Loaded 18500 features\n",
      "Loaded 18600 features\n",
      "Loaded 18700 features\n",
      "Loaded 18800 features\n",
      "Loaded 18900 features\n",
      "Loaded 19000 features\n",
      "Loaded 19100 features\n",
      "Loaded 19200 features\n",
      "Loaded 19300 features\n",
      "Loaded 19400 features\n",
      "Loaded 19500 features\n",
      "Loaded 19600 features\n",
      "Loaded 19700 features\n",
      "Loaded 19800 features\n",
      "Loaded 19900 features\n",
      "Loaded 20000 features\n",
      "Loaded 20100 features\n",
      "Loaded 20200 features\n",
      "Loaded 20300 features\n",
      "Loaded 20400 features\n",
      "Loaded 20500 features\n",
      "Loaded 20600 features\n",
      "Loaded 20700 features\n",
      "Loaded 20800 features\n",
      "Loaded 20900 features\n",
      "Loaded 21000 features\n",
      "Loaded 21100 features\n",
      "Loaded 21200 features\n",
      "Loaded 21300 features\n",
      "Loaded 21400 features\n",
      "Loaded 21500 features\n",
      "Loaded 21600 features\n",
      "Loaded 21700 features\n",
      "Loaded 21800 features\n",
      "Loaded 21900 features\n",
      "Loaded 22000 features\n",
      "Loaded 22100 features\n",
      "Loaded 22200 features\n",
      "Loaded 22300 features\n",
      "Loaded 22400 features\n",
      "Loaded 22500 features\n",
      "Loaded 22600 features\n",
      "Loaded 22700 features\n",
      "Loaded 22800 features\n",
      "Loaded 22900 features\n",
      "Loaded 23000 features\n",
      "Loaded 23100 features\n",
      "Loaded 23200 features\n",
      "Loaded 23300 features\n",
      "Loaded 23400 features\n",
      "Loaded 23500 features\n",
      "Loaded 23600 features\n",
      "Loaded 23700 features\n",
      "Loaded 23800 features\n",
      "Loaded 23900 features\n",
      "Loaded 24000 features\n",
      "Loaded 24100 features\n",
      "Loaded 24200 features\n",
      "Loaded 24300 features\n",
      "Loaded 24400 features\n",
      "Loaded 24500 features\n",
      "Loaded 24600 features\n",
      "Loaded 24700 features\n",
      "Loaded 24800 features\n",
      "Loaded 24900 features\n",
      "Loaded 25000 features\n",
      "Loaded 25100 features\n",
      "Loaded 25200 features\n",
      "Loaded 25300 features\n",
      "Loaded 25400 features\n",
      "Loaded 25500 features\n",
      "Loaded 25600 features\n",
      "Loaded 25700 features\n",
      "Loaded 25800 features\n",
      "Loaded 25900 features\n",
      "Loaded 26000 features\n",
      "Loaded 26100 features\n",
      "Loaded 26200 features\n",
      "Loaded 26300 features\n",
      "Loaded 26400 features\n",
      "Loaded 26500 features\n",
      "Loaded 26600 features\n",
      "Loaded 26700 features\n",
      "Loaded 26800 features\n",
      "Loaded 26900 features\n",
      "Loaded 27000 features\n",
      "Loaded 27100 features\n",
      "Loaded 27200 features\n",
      "Loaded 27300 features\n",
      "Loaded 27400 features\n",
      "Loaded 27500 features\n",
      "Loaded 27600 features\n",
      "Loaded 27700 features\n",
      "Loaded 27800 features\n",
      "Loaded 27900 features\n",
      "Loaded 28000 features\n",
      "Loaded 28100 features\n",
      "Loaded 28200 features\n",
      "Loaded 28300 features\n",
      "Loaded 28400 features\n",
      "Loaded 28500 features\n",
      "Loaded 28600 features\n",
      "Loaded 28700 features\n",
      "Loaded 28800 features\n",
      "Loaded 28900 features\n",
      "Loaded 29000 features\n",
      "Loaded 29100 features\n",
      "Loaded 29200 features\n",
      "Loaded 29300 features\n",
      "Loaded 29400 features\n",
      "Loaded 29500 features\n",
      "Loaded 29600 features\n",
      "Loaded 29700 features\n",
      "Loaded 29800 features\n",
      "Loaded 29900 features\n",
      "Loaded 30000 features\n",
      "Loaded 30100 features\n",
      "Loaded 30200 features\n",
      "Loaded 30300 features\n",
      "Loaded 30400 features\n",
      "Loaded 30500 features\n",
      "Loaded 30600 features\n",
      "Loaded 30700 features\n",
      "Loaded 30800 features\n",
      "Loaded 30900 features\n",
      "Loaded 31000 features\n",
      "Loaded 31100 features\n",
      "Loaded 31200 features\n",
      "Loaded 31300 features\n",
      "Loaded 31400 features\n",
      "Loaded 31500 features\n",
      "Loaded 31600 features\n",
      "Loaded 31700 features\n",
      "Loaded 31800 features\n",
      "Loaded 31900 features\n",
      "Loaded 32000 features\n",
      "Loaded 32100 features\n",
      "Loaded 32200 features\n",
      "Loaded 32300 features\n",
      "Loaded 32400 features\n",
      "Loaded 32500 features\n",
      "Loaded 32600 features\n",
      "Loaded 32700 features\n",
      "Loaded 32800 features\n",
      "Loaded 32900 features\n",
      "Loaded 33000 features\n",
      "Loaded 33100 features\n",
      "Loaded 33200 features\n",
      "Loaded 33300 features\n",
      "Loaded 33400 features\n",
      "Loaded 33500 features\n",
      "Loaded 33600 features\n",
      "Loaded 33700 features\n",
      "Loaded 33800 features\n",
      "Loaded 33900 features\n",
      "Loaded 34000 features\n",
      "Loaded 34100 features\n",
      "Loaded 34200 features\n",
      "Loaded 34300 features\n",
      "Loaded 34400 features\n",
      "Loaded 34500 features\n",
      "Loaded 34600 features\n",
      "Loaded 34700 features\n",
      "Loaded 34800 features\n",
      "Loaded 34900 features\n",
      "Loaded 35000 features\n",
      "Loaded 35100 features\n",
      "Loaded 35200 features\n",
      "Loaded 35300 features\n",
      "Loaded 35400 features\n",
      "Loaded 35500 features\n",
      "Loaded 35600 features\n",
      "Loaded 35700 features\n",
      "Loaded 35800 features\n",
      "Loaded 35900 features\n",
      "Loaded 36000 features\n",
      "Loaded 36100 features\n",
      "Loaded 36200 features\n",
      "Loaded 36300 features\n",
      "Loaded 36400 features\n",
      "Loaded 36500 features\n",
      "Loaded 36600 features\n",
      "Loaded 36700 features\n",
      "Loaded 36800 features\n",
      "Loaded 36900 features\n",
      "Loaded 37000 features\n",
      "Loaded 37100 features\n",
      "Loaded 37200 features\n",
      "Loaded 37300 features\n",
      "Loaded 37400 features\n",
      "Loaded 37500 features\n",
      "Loaded 37600 features\n",
      "Loaded 37700 features\n",
      "Loaded 37800 features\n",
      "Loaded 37900 features\n",
      "Loaded 38000 features\n",
      "Loaded 38100 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 38200 features\n",
      "Loaded 38300 features\n",
      "Loaded 38400 features\n",
      "Loaded 38500 features\n",
      "Loaded 38600 features\n",
      "Loaded 38700 features\n",
      "Loaded 38800 features\n",
      "Loaded 38900 features\n",
      "Loaded 39000 features\n",
      "Loaded 39100 features\n",
      "Loaded 39200 features\n",
      "Loaded 39300 features\n",
      "Loaded 39400 features\n",
      "Loaded 39500 features\n",
      "Loaded 39600 features\n",
      "Loaded 39700 features\n",
      "Loaded 39800 features\n",
      "Loaded 39900 features\n",
      "Loaded 40000 features\n",
      "Loaded 40100 features\n",
      "Loaded 40200 features\n",
      "Loaded 40300 features\n",
      "Loaded 40400 features\n",
      "Loaded 40500 features\n",
      "Loaded 40600 features\n",
      "Loaded 40700 features\n",
      "Loaded 40800 features\n",
      "Loaded 40900 features\n",
      "Loaded 41000 features\n",
      "Loaded 41100 features\n",
      "Loaded 41200 features\n",
      "Loaded 41300 features\n",
      "Loaded 41400 features\n",
      "Loaded 41500 features\n",
      "Loaded 41600 features\n",
      "Loaded 41700 features\n",
      "Loaded 41800 features\n",
      "Loaded 41900 features\n",
      "Loaded 42000 features\n",
      "Loaded 42100 features\n",
      "Loaded 42200 features\n",
      "Loaded 42300 features\n",
      "Loaded 42400 features\n",
      "Loaded 42500 features\n",
      "Loaded 42600 features\n",
      "Loaded 42700 features\n",
      "Loaded 42800 features\n",
      "Loaded 42900 features\n",
      "Loaded 43000 features\n",
      "Loaded 43100 features\n",
      "Loaded 43200 features\n",
      "Loaded 43300 features\n",
      "Loaded 43400 features\n",
      "Loaded 43500 features\n",
      "Loaded 43600 features\n",
      "Loaded 43700 features\n",
      "Loaded 43800 features\n",
      "Loaded 43900 features\n",
      "Loaded 44000 features\n",
      "Loaded 44100 features\n",
      "Loaded 44200 features\n",
      "Loaded 44300 features\n",
      "Loaded 44400 features\n",
      "Loaded 44500 features\n",
      "Loaded 44600 features\n",
      "Loaded 44700 features\n",
      "Loaded 44800 features\n",
      "Loaded 44900 features\n",
      "Loaded 45000 features\n",
      "Loaded 45100 features\n",
      "Loaded 45200 features\n",
      "Loaded 45300 features\n",
      "Loaded 45400 features\n",
      "Loaded 45500 features\n",
      "Loaded 45600 features\n",
      "Loaded 45700 features\n",
      "Loaded 45800 features\n",
      "Loaded 45900 features\n",
      "Loaded 46000 features\n",
      "Loaded 46100 features\n",
      "Loaded 46200 features\n",
      "Loaded 46300 features\n",
      "Loaded 46400 features\n",
      "Loaded 46500 features\n",
      "Loaded 46600 features\n",
      "Loaded 46700 features\n",
      "Loaded 46800 features\n",
      "Loaded 46900 features\n",
      "Loaded 47000 features\n",
      "Loaded 47100 features\n",
      "Loaded 47200 features\n",
      "Loaded 47300 features\n",
      "Loaded 47400 features\n",
      "Loaded 47500 features\n",
      "Loaded 47600 features\n",
      "Loaded 47700 features\n",
      "Loaded 47800 features\n",
      "Loaded 47900 features\n",
      "Loaded 48000 features\n",
      "Loaded 48100 features\n",
      "Loaded 48200 features\n",
      "Loaded 48300 features\n",
      "Loaded 48400 features\n",
      "Loaded 48500 features\n",
      "Loaded 48600 features\n",
      "Loaded 48700 features\n",
      "Loaded 48800 features\n",
      "Loaded 48900 features\n",
      "Loaded 49000 features\n",
      "Loaded 49100 features\n",
      "Loaded 49200 features\n",
      "Loaded 49300 features\n",
      "Loaded 49400 features\n",
      "Loaded 49500 features\n",
      "Loaded 49600 features\n",
      "Loaded 49700 features\n",
      "Loaded 49800 features\n",
      "Loaded 49900 features\n",
      "Loaded 50000 features\n",
      "Loaded 50100 features\n",
      "Loaded 50200 features\n",
      "Loaded 50300 features\n",
      "Loaded 50400 features\n",
      "Loaded 50500 features\n",
      "Loaded 50600 features\n",
      "Loaded 50700 features\n",
      "Loaded 50800 features\n",
      "Loaded 50900 features\n",
      "Loaded 51000 features\n",
      "Loaded 51100 features\n",
      "Loaded 51200 features\n",
      "Loaded 51300 features\n",
      "Loaded 51400 features\n",
      "Loaded 51500 features\n",
      "Loaded 51600 features\n",
      "Loaded 51700 features\n",
      "Loaded 51800 features\n",
      "Loaded 51900 features\n",
      "Loaded 52000 features\n",
      "Loaded 52100 features\n",
      "Loaded 52200 features\n",
      "Loaded 52300 features\n",
      "Loaded 52400 features\n",
      "Loaded 52500 features\n",
      "Loaded 52600 features\n",
      "Loaded 52700 features\n",
      "Loaded 52800 features\n",
      "Loaded 52900 features\n",
      "Loaded 53000 features\n",
      "Loaded 53100 features\n",
      "Loaded 53200 features\n",
      "Loaded 53300 features\n",
      "Loaded 53400 features\n",
      "Loaded 53500 features\n",
      "Loaded 53600 features\n",
      "Loaded 53700 features\n",
      "Loaded 53800 features\n",
      "Loaded 53900 features\n",
      "Loaded 54000 features\n",
      "Loaded 54100 features\n",
      "Loaded 54200 features\n",
      "Loaded 54300 features\n",
      "Loaded 54400 features\n",
      "Loaded 54500 features\n",
      "Loaded 54600 features\n",
      "Loaded 54700 features\n",
      "Loaded 54800 features\n",
      "Loaded 54900 features\n",
      "Loaded 55000 features\n",
      "Loaded 55100 features\n",
      "Loaded 55200 features\n",
      "Loaded 55300 features\n",
      "Loaded 55400 features\n",
      "Loaded 55500 features\n",
      "Loaded 55600 features\n",
      "Loaded 55700 features\n",
      "Loaded 55800 features\n",
      "Loaded 55900 features\n",
      "Loaded 56000 features\n",
      "Loaded 56100 features\n",
      "Loaded 56200 features\n",
      "Loaded 56300 features\n",
      "Loaded 56400 features\n",
      "Loaded 56500 features\n",
      "Loaded 56600 features\n",
      "Loaded 56700 features\n",
      "Loaded 56800 features\n",
      "Loaded 56900 features\n",
      "Loaded 57000 features\n",
      "Loaded 57100 features\n",
      "Loaded 57200 features\n",
      "Loaded 57300 features\n",
      "Loaded 57400 features\n",
      "Loaded 57500 features\n",
      "Loaded 57600 features\n",
      "Loaded 57700 features\n",
      "Loaded 57800 features\n",
      "Loaded 57900 features\n",
      "Loaded 58000 features\n",
      "Loaded 58100 features\n",
      "Loaded 58200 features\n",
      "Loaded 58300 features\n",
      "Loaded 58400 features\n",
      "Loaded 58500 features\n",
      "Loaded 58600 features\n",
      "Loaded 58700 features\n",
      "Loaded 58800 features\n",
      "Loaded 58900 features\n",
      "Loaded 59000 features\n",
      "Loaded 59100 features\n",
      "Loaded 59200 features\n",
      "Loaded 59300 features\n",
      "Loaded 59400 features\n",
      "Loaded 59500 features\n",
      "Loaded 59600 features\n",
      "Loaded 59700 features\n",
      "Loaded 59800 features\n",
      "Loaded 59900 features\n",
      "Loaded 60000 features\n",
      "Loaded 60100 features\n",
      "Loaded 60200 features\n",
      "Loaded 60300 features\n",
      "Loaded 60400 features\n",
      "Loaded 60500 features\n",
      "Loaded 60600 features\n",
      "Loaded 60700 features\n",
      "Loaded 60800 features\n",
      "Loaded 60900 features\n",
      "Loaded 61000 features\n",
      "Loaded 61100 features\n",
      "Loaded 61200 features\n",
      "Loaded 61300 features\n",
      "Loaded 61400 features\n",
      "Loaded 61500 features\n",
      "Loaded 61600 features\n",
      "Loaded 61700 features\n",
      "Loaded 61800 features\n",
      "Loaded 61900 features\n",
      "Loaded 62000 features\n",
      "Loaded 62100 features\n",
      "Loaded 62200 features\n",
      "Loaded 62300 features\n",
      "Loaded 62400 features\n",
      "Loaded 62500 features\n",
      "Loaded 62600 features\n",
      "Loaded 62700 features\n",
      "Loaded 62800 features\n",
      "Loaded 62900 features\n",
      "Loaded 63000 features\n",
      "Loaded 63100 features\n",
      "Loaded 63200 features\n",
      "Loaded 63300 features\n",
      "Loaded 63400 features\n",
      "Loaded 63500 features\n",
      "Loaded 63600 features\n",
      "Loaded 63700 features\n",
      "Loaded 63800 features\n",
      "Loaded 63900 features\n",
      "Loaded 64000 features\n",
      "Loaded 64100 features\n",
      "Loaded 64200 features\n",
      "Loaded 64300 features\n",
      "Loaded 64400 features\n",
      "Loaded 64500 features\n",
      "Loaded 64600 features\n",
      "Loaded 64700 features\n",
      "Loaded 64800 features\n",
      "Loaded 64900 features\n",
      "Loaded 65000 features\n",
      "Loaded 65100 features\n",
      "Loaded 65200 features\n",
      "Loaded 65300 features\n",
      "Loaded 65400 features\n",
      "Loaded 65500 features\n",
      "Loaded 65600 features\n",
      "Loaded 65700 features\n",
      "Loaded 65800 features\n",
      "Loaded 65900 features\n",
      "Loaded 66000 features\n",
      "Loaded 66100 features\n",
      "Loaded 66200 features\n",
      "Loaded 66300 features\n",
      "Loaded 66400 features\n",
      "Loaded 66500 features\n",
      "Loaded 66600 features\n",
      "Loaded 66700 features\n",
      "Loaded 66800 features\n",
      "Loaded 66900 features\n",
      "Loaded 67000 features\n",
      "Loaded 67100 features\n",
      "Loaded 67200 features\n",
      "Loaded 67300 features\n",
      "Loaded 67400 features\n",
      "Loaded 67500 features\n",
      "Loaded 67600 features\n",
      "Loaded 67700 features\n",
      "Loaded 67800 features\n",
      "Loaded 67900 features\n",
      "Loaded 68000 features\n",
      "Loaded 68100 features\n",
      "Loaded 68200 features\n",
      "Loaded 68300 features\n",
      "Loaded 68400 features\n",
      "Loaded 68500 features\n",
      "Loaded 68600 features\n",
      "Loaded 68700 features\n",
      "Loaded 68800 features\n",
      "Loaded 68900 features\n",
      "Loaded 69000 features\n",
      "Loaded 69100 features\n",
      "Loaded 69200 features\n",
      "Loaded 69300 features\n",
      "Loaded 69400 features\n",
      "Loaded 69500 features\n",
      "Loaded 69600 features\n",
      "Loaded 69700 features\n",
      "Loaded 69800 features\n",
      "Loaded 69900 features\n",
      "Loaded 70000 features\n",
      "Loaded 70100 features\n",
      "Loaded 70200 features\n",
      "Loaded 70300 features\n",
      "Loaded 70400 features\n",
      "Loaded 70500 features\n",
      "Loaded 70600 features\n",
      "Loaded 70700 features\n",
      "Loaded 70800 features\n",
      "Loaded 70900 features\n",
      "Loaded 71000 features\n",
      "Loaded 71100 features\n",
      "Loaded 71200 features\n",
      "Loaded 71300 features\n",
      "Loaded 71400 features\n",
      "Loaded 71500 features\n",
      "Loaded 71600 features\n",
      "Loaded 71700 features\n",
      "Loaded 71800 features\n",
      "Loaded 71900 features\n",
      "Loaded 72000 features\n",
      "Loaded 72100 features\n",
      "Loaded 72200 features\n",
      "Loaded 72300 features\n",
      "Loaded 72400 features\n",
      "Loaded 72500 features\n",
      "Loaded 72600 features\n",
      "Loaded 72700 features\n",
      "Loaded 72800 features\n",
      "Loaded 72900 features\n",
      "Loaded 73000 features\n",
      "Loaded 73100 features\n",
      "Loaded 73200 features\n",
      "Loaded 73300 features\n",
      "Loaded 73400 features\n",
      "Loaded 73500 features\n",
      "Loaded 73600 features\n",
      "Loaded 73700 features\n",
      "Loaded 73800 features\n",
      "Loaded 73900 features\n",
      "Loaded 74000 features\n",
      "Loaded 74100 features\n",
      "Loaded 74200 features\n",
      "Loaded 74300 features\n",
      "Loaded 74400 features\n",
      "Loaded 74500 features\n",
      "Loaded 74600 features\n",
      "Loaded 74700 features\n",
      "Loaded 74800 features\n",
      "Loaded 74900 features\n",
      "Loaded 75000 features\n",
      "Loaded 75100 features\n",
      "Loaded 75200 features\n",
      "Loaded 75300 features\n",
      "Loaded 75400 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75500 features\n",
      "Loaded 75600 features\n",
      "Loaded 75700 features\n",
      "Loaded 75800 features\n",
      "Loaded 75900 features\n",
      "Loaded 76000 features\n",
      "Loaded 76100 features\n",
      "Loaded 76200 features\n",
      "Loaded 76300 features\n",
      "Loaded 76400 features\n",
      "Loaded a total of 76407 features\n",
      "Variable 'feature_list' contains 76407 features\n"
     ]
    }
   ],
   "source": [
    "#    This code loads file, or a small amount of the complete dataset\n",
    "MAX_FEATURE_LOAD = 500\n",
    "feature_list = list()\n",
    "feature_count = 0\n",
    "# A rewind() on the reader object makes sure we're at the start of the feature list\n",
    "# This way, we can execute this cell multiple times and get the same result \n",
    "wfreader.rewind()\n",
    "# Now, read through each of the features, saving them as dictionaries into a list\n",
    "feature = wfreader.next()\n",
    "while feature:\n",
    "    feature_list.append(feature)\n",
    "    feature_count += 1\n",
    "    # if we're loading a lot of features, print progress\n",
    "    if (feature_count % 100) == 0:\n",
    "        print(f\"Loaded {feature_count} features\")\n",
    "    # loaded the max we're allowed then break\n",
    "#     if feature_count >= MAX_FEATURE_LOAD:\n",
    "#         break\n",
    "    feature = wfreader.next()\n",
    "#\n",
    "#    Print the number of items (features) we think we loaded\n",
    "print(f\"Loaded a total of {feature_count} features\")\n",
    "#\n",
    "#    Just a validation check - did all the items we loaded get into the list?\n",
    "print(f\"Variable 'feature_list' contains {len(feature_list)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fb375",
   "metadata": {},
   "source": [
    "## Create an Estimate for the Smoke Severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb64195",
   "metadata": {},
   "source": [
    "The next task is to design a smoke estimate and apply it to my fires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f1352",
   "metadata": {},
   "source": [
    "**Description of my Smoke Estimate:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6ff6a",
   "metadata": {},
   "source": [
    "Our next task is to create a smoke estimate.\n",
    "First, I consider what will I allow for my range of values. According to airnow.gov, the AQI index ranges from 0 to 500, where 0 is the cleanest air and 500 is the worst [10]. Now, because the assignment requires me to compare my smoke estimate to the AQI information from the EPA, I would like to try to encourage a similar range for most of my estimates. \n",
    "\n",
    "Next, I consider what factors I want to be incorporated into my smoke severity estimate. The assignment instructions provide several helpful ideas saying \"It seems reasonable that a large fire, that burns a large number of acres, and that is close to a city would put more smoke into a city than a small fire that is much further away.\" After giving it some thought, I will base my estimate off 1). each fire's size, measured in the number of acres burned, and 2). its distance from Richland as calculated earlier above.\n",
    "\n",
    "Let's think about each of these factors, starting with acres burned. Now, it makes sense that my smoke estimate should be increase for larger fires and decrease for smaller fires, all else being equal. Additionally, a fire closer to Richland should result in poorer smoke conditions than a fire far away. Taken together, these two factors are at odds with each other, and so I decided to take their ratio for my smoke estimate - yielding: ***acres_burned / distance_from_city***.\n",
    "\n",
    "There are several more changes to make to this estimate to improve it a bit. First, we can convert the acres burned to square miles burned so that the numerator and denominator have the same units. As an aside, the resulting units are now 1/mi, and to be honest I am not sure how to think about a smoke estimator with a unit attached to it, so for the sake of this assignment, I will not concern myself with the estimate's units. \n",
    "\n",
    "While designing my estimator, I revisited the course lecture notes from 10-30-23 and looked at the professor's findings from his exporatory data analysis. According to lecture, about 94% of all fires are between 0 and 5000 acres (approx. 7.8 square miles), but the largest was 1.4M acres (approx. 2,100 square miles). In contrast, the distances I will consider will be no greater than 1,250 miles from Richland. Because the majority of these areas burned are small relative to the distances, I added a scaling factor to the numerator so it will be weighted more evenly, by multiplying by 100. \n",
    "\n",
    "I also wanted to reduce the frequency of more extreme values in my smoke estimates, and so I enforce a heuristic that the minimum distance a fire can be from the city is 1 mile, and that any fire closer than that is treated by the estimate as 1 mile. Simillarly, I enforce a minimum size of burned area (0.5 square miles). Lastly, I want to ensure my estimate has a minimum value of 1, so I take the argmax of the expression and thus my final estimate is:\n",
    "$$argmax(\\frac{\\text{(square_mi_burned * 100)}}{\\text{distance_from_city}}, 1)$$\n",
    "\n",
    "Below is the function I defined to calculate my smoke estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e1d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance_from_city should be provided in miles\n",
    "def calc_smoke_severity_est(acres_burned, distance_from_city):\n",
    "    SCALING_FACTOR = 100\n",
    "    \n",
    "    #convert acres to square miles. I got formula from Google [10]\n",
    "    square_mi_burned = acres_burned / 640\n",
    "#     print(\"acres_burned {}\".format(acres_burned))\n",
    "#     print(\"square_mi_burned {}\".format(square_mi_burned))\n",
    "    \n",
    "    #to simplify the estimate, set 1 as the minimum\n",
    "    #distance and 0.5 as the minimum square miles burned\n",
    "#     print(\"distance from Richland {}\".format(distance_from_city))\n",
    "    if distance_from_city < 1:\n",
    "        distance_from_city = 1\n",
    "    \n",
    "    if square_mi_burned < 0.5:\n",
    "        square_mi_burned = 0.5\n",
    "    \n",
    "    # I learned how to do argmax() in python from [12] and [13]\n",
    "    value = (square_mi_burned * SCALING_FACTOR) / distance_from_city\n",
    "    options = [value, 1]\n",
    "    index = np.argmax([(square_mi_burned * SCALING_FACTOR) / distance_from_city, 1])\n",
    "    result = options[index]\n",
    "#     print(\"Result: {}\".format(result))\n",
    "#     print('')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f0b9c",
   "metadata": {},
   "source": [
    "Now, let's apply the function to my list of relevant fires and store the results in a dataframe. First we convert the list of fires to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe1c6e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>USGS_Assigned_ID</th>\n",
       "      <th>Assigned_Fire_Type</th>\n",
       "      <th>Fire_Year</th>\n",
       "      <th>Fire_Polygon_Tier</th>\n",
       "      <th>Fire_Attribute_Tiers</th>\n",
       "      <th>GIS_Acres</th>\n",
       "      <th>GIS_Hectares</th>\n",
       "      <th>Source_Datasets</th>\n",
       "      <th>Listed_Fire_Types</th>\n",
       "      <th>...</th>\n",
       "      <th>Wildfire_Notice</th>\n",
       "      <th>Prescribed_Burn_Notice</th>\n",
       "      <th>Wildfire_and_Rx_Flag</th>\n",
       "      <th>Overlap_Within_1_or_2_Flag</th>\n",
       "      <th>Circleness_Scale</th>\n",
       "      <th>Circle_Flag</th>\n",
       "      <th>Exclude_From_Summary_Rasters</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>distance_to_richland</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14299</td>\n",
       "      <td>14299</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3)</td>\n",
       "      <td>40992.458271</td>\n",
       "      <td>16589.059302</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (1), Likely Wildfire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.385355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>73550.428118</td>\n",
       "      <td>1.658906e+08</td>\n",
       "      <td>189.757596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14300</td>\n",
       "      <td>14300</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3)</td>\n",
       "      <td>25757.090203</td>\n",
       "      <td>10423.524591</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (2), Likely Wildfire (2)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.364815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>59920.576713</td>\n",
       "      <td>1.042352e+08</td>\n",
       "      <td>163.213533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14301</td>\n",
       "      <td>14301</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (5), 3 (15), 5 (1)</td>\n",
       "      <td>45527.210986</td>\n",
       "      <td>18424.208617</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (6), Likely Wildfire (15)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.320927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>84936.827810</td>\n",
       "      <td>1.842421e+08</td>\n",
       "      <td>190.823448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14302</td>\n",
       "      <td>14302</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3), 5 (1)</td>\n",
       "      <td>10395.010334</td>\n",
       "      <td>4206.711433</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (2), Likely Wildfire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.428936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>35105.903602</td>\n",
       "      <td>4.206711e+07</td>\n",
       "      <td>273.615572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14303</td>\n",
       "      <td>14303</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3)</td>\n",
       "      <td>9983.605738</td>\n",
       "      <td>4040.221900</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (1), Likely Wildfire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.703178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>26870.456126</td>\n",
       "      <td>4.040222e+07</td>\n",
       "      <td>205.770574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76402</th>\n",
       "      <td>135057</td>\n",
       "      <td>135057</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (3)</td>\n",
       "      <td>16.412148</td>\n",
       "      <td>6.641761</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.177425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2168.900740</td>\n",
       "      <td>6.641761e+04</td>\n",
       "      <td>248.914999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76403</th>\n",
       "      <td>135058</td>\n",
       "      <td>135058</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (1)</td>\n",
       "      <td>7.050837</td>\n",
       "      <td>2.853373</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (1)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.374368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>978.666221</td>\n",
       "      <td>2.853373e+04</td>\n",
       "      <td>167.983467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76404</th>\n",
       "      <td>135059</td>\n",
       "      <td>135059</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (4)</td>\n",
       "      <td>9.342668</td>\n",
       "      <td>3.780843</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (4)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1958.326660</td>\n",
       "      <td>3.780843e+04</td>\n",
       "      <td>168.822089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76405</th>\n",
       "      <td>135060</td>\n",
       "      <td>135060</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (1)</td>\n",
       "      <td>0.996962</td>\n",
       "      <td>0.403456</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (1)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.993809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>225.866452</td>\n",
       "      <td>4.034562e+03</td>\n",
       "      <td>655.423767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76406</th>\n",
       "      <td>135061</td>\n",
       "      <td>135061</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (1)</td>\n",
       "      <td>0.969953</td>\n",
       "      <td>0.392526</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (1)</td>\n",
       "      <td>...</td>\n",
       "      <td>Wildfire mapping prior to 1984 was inconsisten...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.744794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>257.348237</td>\n",
       "      <td>3.925261e+03</td>\n",
       "      <td>224.477677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76407 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OBJECTID  USGS_Assigned_ID Assigned_Fire_Type  Fire_Year  \\\n",
       "0         14299             14299           Wildfire       1963   \n",
       "1         14300             14300           Wildfire       1963   \n",
       "2         14301             14301           Wildfire       1963   \n",
       "3         14302             14302           Wildfire       1963   \n",
       "4         14303             14303           Wildfire       1963   \n",
       "...         ...               ...                ...        ...   \n",
       "76402    135057            135057    Prescribed Fire       2020   \n",
       "76403    135058            135058    Prescribed Fire       2020   \n",
       "76404    135059            135059    Prescribed Fire       2020   \n",
       "76405    135060            135060    Prescribed Fire       2020   \n",
       "76406    135061            135061    Prescribed Fire       2020   \n",
       "\n",
       "       Fire_Polygon_Tier  Fire_Attribute_Tiers     GIS_Acres  GIS_Hectares  \\\n",
       "0                      1          1 (1), 3 (3)  40992.458271  16589.059302   \n",
       "1                      1          1 (1), 3 (3)  25757.090203  10423.524591   \n",
       "2                      1  1 (5), 3 (15), 5 (1)  45527.210986  18424.208617   \n",
       "3                      1   1 (1), 3 (3), 5 (1)  10395.010334   4206.711433   \n",
       "4                      1          1 (1), 3 (3)   9983.605738   4040.221900   \n",
       "...                  ...                   ...           ...           ...   \n",
       "76402                  8                 8 (3)     16.412148      6.641761   \n",
       "76403                  8                 8 (1)      7.050837      2.853373   \n",
       "76404                  8                 8 (4)      9.342668      3.780843   \n",
       "76405                  8                 8 (1)      0.996962      0.403456   \n",
       "76406                  8                 8 (1)      0.969953      0.392526   \n",
       "\n",
       "                                         Source_Datasets  \\\n",
       "0      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "1      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "2      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "3      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "4      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "...                                                  ...   \n",
       "76402  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76403  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76404  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76405  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76406  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "\n",
       "                        Listed_Fire_Types  ...  \\\n",
       "0       Wildfire (1), Likely Wildfire (3)  ...   \n",
       "1       Wildfire (2), Likely Wildfire (2)  ...   \n",
       "2      Wildfire (6), Likely Wildfire (15)  ...   \n",
       "3       Wildfire (2), Likely Wildfire (3)  ...   \n",
       "4       Wildfire (1), Likely Wildfire (3)  ...   \n",
       "...                                   ...  ...   \n",
       "76402                 Prescribed Fire (3)  ...   \n",
       "76403                 Prescribed Fire (1)  ...   \n",
       "76404                 Prescribed Fire (4)  ...   \n",
       "76405                 Prescribed Fire (1)  ...   \n",
       "76406                 Prescribed Fire (1)  ...   \n",
       "\n",
       "                                         Wildfire_Notice  \\\n",
       "0      Wildfire mapping prior to 1984 was inconsisten...   \n",
       "1      Wildfire mapping prior to 1984 was inconsisten...   \n",
       "2      Wildfire mapping prior to 1984 was inconsisten...   \n",
       "3      Wildfire mapping prior to 1984 was inconsisten...   \n",
       "4      Wildfire mapping prior to 1984 was inconsisten...   \n",
       "...                                                  ...   \n",
       "76402  Wildfire mapping prior to 1984 was inconsisten...   \n",
       "76403  Wildfire mapping prior to 1984 was inconsisten...   \n",
       "76404  Wildfire mapping prior to 1984 was inconsisten...   \n",
       "76405  Wildfire mapping prior to 1984 was inconsisten...   \n",
       "76406  Wildfire mapping prior to 1984 was inconsisten...   \n",
       "\n",
       "                                  Prescribed_Burn_Notice Wildfire_and_Rx_Flag  \\\n",
       "0      Prescribed fire data in this dataset represent...                 None   \n",
       "1      Prescribed fire data in this dataset represent...                 None   \n",
       "2      Prescribed fire data in this dataset represent...                 None   \n",
       "3      Prescribed fire data in this dataset represent...                 None   \n",
       "4      Prescribed fire data in this dataset represent...                 None   \n",
       "...                                                  ...                  ...   \n",
       "76402  Prescribed fire data in this dataset represent...                 None   \n",
       "76403  Prescribed fire data in this dataset represent...                 None   \n",
       "76404  Prescribed fire data in this dataset represent...                 None   \n",
       "76405  Prescribed fire data in this dataset represent...                 None   \n",
       "76406  Prescribed fire data in this dataset represent...                 None   \n",
       "\n",
       "                              Overlap_Within_1_or_2_Flag Circleness_Scale  \\\n",
       "0                                                   None         0.385355   \n",
       "1                                                   None         0.364815   \n",
       "2                                                   None         0.320927   \n",
       "3                                                   None         0.428936   \n",
       "4                                                   None         0.703178   \n",
       "...                                                  ...              ...   \n",
       "76402  Caution, this Prescribed Fire in 2020 overlaps...         0.177425   \n",
       "76403  Caution, this Prescribed Fire in 2020 overlaps...         0.374368   \n",
       "76404  Caution, this Prescribed Fire in 2020 overlaps...         0.123888   \n",
       "76405                                               None         0.993809   \n",
       "76406  Caution, this Prescribed Fire in 2020 overlaps...         0.744794   \n",
       "\n",
       "      Circle_Flag Exclude_From_Summary_Rasters  Shape_Length    Shape_Area  \\\n",
       "0             NaN                           No  73550.428118  1.658906e+08   \n",
       "1             NaN                           No  59920.576713  1.042352e+08   \n",
       "2             NaN                           No  84936.827810  1.842421e+08   \n",
       "3             NaN                           No  35105.903602  4.206711e+07   \n",
       "4             NaN                           No  26870.456126  4.040222e+07   \n",
       "...           ...                          ...           ...           ...   \n",
       "76402         NaN                           No   2168.900740  6.641761e+04   \n",
       "76403         NaN                           No    978.666221  2.853373e+04   \n",
       "76404         NaN                           No   1958.326660  3.780843e+04   \n",
       "76405         1.0                           No    225.866452  4.034562e+03   \n",
       "76406         NaN                           No    257.348237  3.925261e+03   \n",
       "\n",
       "      distance_to_richland  \n",
       "0               189.757596  \n",
       "1               163.213533  \n",
       "2               190.823448  \n",
       "3               273.615572  \n",
       "4               205.770574  \n",
       "...                    ...  \n",
       "76402           248.914999  \n",
       "76403           167.983467  \n",
       "76404           168.822089  \n",
       "76405           655.423767  \n",
       "76406           224.477677  \n",
       "\n",
       "[76407 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consulted for how to create the dataframe:\n",
    "# https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-list-of-dicts/#\n",
    "feature_list\n",
    "df_fires = pd.DataFrame.from_records(feature_list)\n",
    "df_fires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0447871",
   "metadata": {},
   "source": [
    "Next we apply our smoke estimate and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebff42d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>USGS_Assigned_ID</th>\n",
       "      <th>Assigned_Fire_Type</th>\n",
       "      <th>Fire_Year</th>\n",
       "      <th>Fire_Polygon_Tier</th>\n",
       "      <th>Fire_Attribute_Tiers</th>\n",
       "      <th>GIS_Acres</th>\n",
       "      <th>GIS_Hectares</th>\n",
       "      <th>Source_Datasets</th>\n",
       "      <th>Listed_Fire_Types</th>\n",
       "      <th>...</th>\n",
       "      <th>Prescribed_Burn_Notice</th>\n",
       "      <th>Wildfire_and_Rx_Flag</th>\n",
       "      <th>Overlap_Within_1_or_2_Flag</th>\n",
       "      <th>Circleness_Scale</th>\n",
       "      <th>Circle_Flag</th>\n",
       "      <th>Exclude_From_Summary_Rasters</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>distance_to_richland</th>\n",
       "      <th>my_smoke_estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14299</td>\n",
       "      <td>14299</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3)</td>\n",
       "      <td>40992.458271</td>\n",
       "      <td>16589.059302</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (1), Likely Wildfire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.385355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>73550.428118</td>\n",
       "      <td>1.658906e+08</td>\n",
       "      <td>189.757596</td>\n",
       "      <td>33.753967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14300</td>\n",
       "      <td>14300</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3)</td>\n",
       "      <td>25757.090203</td>\n",
       "      <td>10423.524591</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (2), Likely Wildfire (2)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.364815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>59920.576713</td>\n",
       "      <td>1.042352e+08</td>\n",
       "      <td>163.213533</td>\n",
       "      <td>24.658160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14301</td>\n",
       "      <td>14301</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (5), 3 (15), 5 (1)</td>\n",
       "      <td>45527.210986</td>\n",
       "      <td>18424.208617</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (6), Likely Wildfire (15)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.320927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>84936.827810</td>\n",
       "      <td>1.842421e+08</td>\n",
       "      <td>190.823448</td>\n",
       "      <td>37.278578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14302</td>\n",
       "      <td>14302</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3), 5 (1)</td>\n",
       "      <td>10395.010334</td>\n",
       "      <td>4206.711433</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (2), Likely Wildfire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.428936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>35105.903602</td>\n",
       "      <td>4.206711e+07</td>\n",
       "      <td>273.615572</td>\n",
       "      <td>5.936140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14303</td>\n",
       "      <td>14303</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>1963</td>\n",
       "      <td>1</td>\n",
       "      <td>1 (1), 3 (3)</td>\n",
       "      <td>9983.605738</td>\n",
       "      <td>4040.221900</td>\n",
       "      <td>Comb_National_NIFC_Interagency_Fire_Perimeter_...</td>\n",
       "      <td>Wildfire (1), Likely Wildfire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.703178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>26870.456126</td>\n",
       "      <td>4.040222e+07</td>\n",
       "      <td>205.770574</td>\n",
       "      <td>7.580960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76402</th>\n",
       "      <td>135057</td>\n",
       "      <td>135057</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (3)</td>\n",
       "      <td>16.412148</td>\n",
       "      <td>6.641761</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.177425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>2168.900740</td>\n",
       "      <td>6.641761e+04</td>\n",
       "      <td>248.914999</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76403</th>\n",
       "      <td>135058</td>\n",
       "      <td>135058</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (1)</td>\n",
       "      <td>7.050837</td>\n",
       "      <td>2.853373</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (1)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.374368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>978.666221</td>\n",
       "      <td>2.853373e+04</td>\n",
       "      <td>167.983467</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76404</th>\n",
       "      <td>135059</td>\n",
       "      <td>135059</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (4)</td>\n",
       "      <td>9.342668</td>\n",
       "      <td>3.780843</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (4)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1958.326660</td>\n",
       "      <td>3.780843e+04</td>\n",
       "      <td>168.822089</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76405</th>\n",
       "      <td>135060</td>\n",
       "      <td>135060</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (1)</td>\n",
       "      <td>0.996962</td>\n",
       "      <td>0.403456</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (1)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.993809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>225.866452</td>\n",
       "      <td>4.034562e+03</td>\n",
       "      <td>655.423767</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76406</th>\n",
       "      <td>135061</td>\n",
       "      <td>135061</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>8 (1)</td>\n",
       "      <td>0.969953</td>\n",
       "      <td>0.392526</td>\n",
       "      <td>Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...</td>\n",
       "      <td>Prescribed Fire (1)</td>\n",
       "      <td>...</td>\n",
       "      <td>Prescribed fire data in this dataset represent...</td>\n",
       "      <td>None</td>\n",
       "      <td>Caution, this Prescribed Fire in 2020 overlaps...</td>\n",
       "      <td>0.744794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>257.348237</td>\n",
       "      <td>3.925261e+03</td>\n",
       "      <td>224.477677</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76407 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       OBJECTID  USGS_Assigned_ID Assigned_Fire_Type  Fire_Year  \\\n",
       "0         14299             14299           Wildfire       1963   \n",
       "1         14300             14300           Wildfire       1963   \n",
       "2         14301             14301           Wildfire       1963   \n",
       "3         14302             14302           Wildfire       1963   \n",
       "4         14303             14303           Wildfire       1963   \n",
       "...         ...               ...                ...        ...   \n",
       "76402    135057            135057    Prescribed Fire       2020   \n",
       "76403    135058            135058    Prescribed Fire       2020   \n",
       "76404    135059            135059    Prescribed Fire       2020   \n",
       "76405    135060            135060    Prescribed Fire       2020   \n",
       "76406    135061            135061    Prescribed Fire       2020   \n",
       "\n",
       "       Fire_Polygon_Tier  Fire_Attribute_Tiers     GIS_Acres  GIS_Hectares  \\\n",
       "0                      1          1 (1), 3 (3)  40992.458271  16589.059302   \n",
       "1                      1          1 (1), 3 (3)  25757.090203  10423.524591   \n",
       "2                      1  1 (5), 3 (15), 5 (1)  45527.210986  18424.208617   \n",
       "3                      1   1 (1), 3 (3), 5 (1)  10395.010334   4206.711433   \n",
       "4                      1          1 (1), 3 (3)   9983.605738   4040.221900   \n",
       "...                  ...                   ...           ...           ...   \n",
       "76402                  8                 8 (3)     16.412148      6.641761   \n",
       "76403                  8                 8 (1)      7.050837      2.853373   \n",
       "76404                  8                 8 (4)      9.342668      3.780843   \n",
       "76405                  8                 8 (1)      0.996962      0.403456   \n",
       "76406                  8                 8 (1)      0.969953      0.392526   \n",
       "\n",
       "                                         Source_Datasets  \\\n",
       "0      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "1      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "2      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "3      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "4      Comb_National_NIFC_Interagency_Fire_Perimeter_...   \n",
       "...                                                  ...   \n",
       "76402  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76403  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76404  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76405  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "76406  Comb_National_Rx_Only_BLM_VTRT_Prescribed_Fire...   \n",
       "\n",
       "                        Listed_Fire_Types  ...  \\\n",
       "0       Wildfire (1), Likely Wildfire (3)  ...   \n",
       "1       Wildfire (2), Likely Wildfire (2)  ...   \n",
       "2      Wildfire (6), Likely Wildfire (15)  ...   \n",
       "3       Wildfire (2), Likely Wildfire (3)  ...   \n",
       "4       Wildfire (1), Likely Wildfire (3)  ...   \n",
       "...                                   ...  ...   \n",
       "76402                 Prescribed Fire (3)  ...   \n",
       "76403                 Prescribed Fire (1)  ...   \n",
       "76404                 Prescribed Fire (4)  ...   \n",
       "76405                 Prescribed Fire (1)  ...   \n",
       "76406                 Prescribed Fire (1)  ...   \n",
       "\n",
       "                                  Prescribed_Burn_Notice Wildfire_and_Rx_Flag  \\\n",
       "0      Prescribed fire data in this dataset represent...                 None   \n",
       "1      Prescribed fire data in this dataset represent...                 None   \n",
       "2      Prescribed fire data in this dataset represent...                 None   \n",
       "3      Prescribed fire data in this dataset represent...                 None   \n",
       "4      Prescribed fire data in this dataset represent...                 None   \n",
       "...                                                  ...                  ...   \n",
       "76402  Prescribed fire data in this dataset represent...                 None   \n",
       "76403  Prescribed fire data in this dataset represent...                 None   \n",
       "76404  Prescribed fire data in this dataset represent...                 None   \n",
       "76405  Prescribed fire data in this dataset represent...                 None   \n",
       "76406  Prescribed fire data in this dataset represent...                 None   \n",
       "\n",
       "                              Overlap_Within_1_or_2_Flag Circleness_Scale  \\\n",
       "0                                                   None         0.385355   \n",
       "1                                                   None         0.364815   \n",
       "2                                                   None         0.320927   \n",
       "3                                                   None         0.428936   \n",
       "4                                                   None         0.703178   \n",
       "...                                                  ...              ...   \n",
       "76402  Caution, this Prescribed Fire in 2020 overlaps...         0.177425   \n",
       "76403  Caution, this Prescribed Fire in 2020 overlaps...         0.374368   \n",
       "76404  Caution, this Prescribed Fire in 2020 overlaps...         0.123888   \n",
       "76405                                               None         0.993809   \n",
       "76406  Caution, this Prescribed Fire in 2020 overlaps...         0.744794   \n",
       "\n",
       "      Circle_Flag Exclude_From_Summary_Rasters  Shape_Length    Shape_Area  \\\n",
       "0             NaN                           No  73550.428118  1.658906e+08   \n",
       "1             NaN                           No  59920.576713  1.042352e+08   \n",
       "2             NaN                           No  84936.827810  1.842421e+08   \n",
       "3             NaN                           No  35105.903602  4.206711e+07   \n",
       "4             NaN                           No  26870.456126  4.040222e+07   \n",
       "...           ...                          ...           ...           ...   \n",
       "76402         NaN                           No   2168.900740  6.641761e+04   \n",
       "76403         NaN                           No    978.666221  2.853373e+04   \n",
       "76404         NaN                           No   1958.326660  3.780843e+04   \n",
       "76405         1.0                           No    225.866452  4.034562e+03   \n",
       "76406         NaN                           No    257.348237  3.925261e+03   \n",
       "\n",
       "      distance_to_richland my_smoke_estimate  \n",
       "0               189.757596         33.753967  \n",
       "1               163.213533         24.658160  \n",
       "2               190.823448         37.278578  \n",
       "3               273.615572          5.936140  \n",
       "4               205.770574          7.580960  \n",
       "...                    ...               ...  \n",
       "76402           248.914999          1.000000  \n",
       "76403           167.983467          1.000000  \n",
       "76404           168.822089          1.000000  \n",
       "76405           655.423767          1.000000  \n",
       "76406           224.477677          1.000000  \n",
       "\n",
       "[76407 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I tried to vectorize this but couldn't figure it out: \n",
    "# https://tryolabs.com/blog/2023/02/08/top-5-tips-to-make-your-pandas-code-absurdly-fast\n",
    "\n",
    "#https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "# recommends using apply, but I used the webpage to learn how to use iterrows anyway.\n",
    "\n",
    "# df_fires['my_smoke_estimate'] = calc_smoke_severity_est(df_fires['GIS_Acres'],\n",
    "#                                                         df_fires['distance_to_richland'])\n",
    "smoke_est_list = []\n",
    "\n",
    "for index, row in df_fires.iterrows():\n",
    "    smoke_est = calc_smoke_severity_est(row['GIS_Acres'], row['distance_to_richland'])\n",
    "    smoke_est_list.append(smoke_est)\n",
    "    \n",
    "#add column to dataframe\n",
    "df_fires['my_smoke_estimate'] = smoke_est_list\n",
    "df_fires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485b3fd",
   "metadata": {},
   "source": [
    "## Compare My Smoke Estimate with US EPA's AQI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13908431",
   "metadata": {},
   "source": [
    "### Acquire AQI Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d80214",
   "metadata": {},
   "source": [
    "The first thing we need to do to compare my smoke estimate with the AQI data is to acquire the AQI data. **Note, for many of the subsequent steps, I base my work heavily upon Dr. McDonald's AQI example notebook [14].** \n",
    "\n",
    "First, I directly copied a cell from [14] to define useful constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a4d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#\n",
    "#    This is the root of all AQS API URLs\n",
    "#\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "#\n",
    "#    These are 'actions' we can ask the API to take or requests that we can make of the API\n",
    "#\n",
    "#    Sign-up request - generally only performed once - unless you lose your key\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "#\n",
    "#    List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "#\n",
    "#    Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    It is always nice to be respectful of a free data resource.\n",
    "#    We're going to observe a 100 requests per minute limit - which is fairly nice\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "#\n",
    "#\n",
    "#    This is a template that covers most of the parameters for the actions we might take, from the set of actions\n",
    "#    above. In the examples below, most of the time parameters can either be supplied as individual values to a\n",
    "#    function - or they can be set in a copy of the template and passed in with the template.\n",
    "# \n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229aaa5b",
   "metadata": {},
   "source": [
    "Next, I again reuse a cell from [14] to define a function that requests data from the EPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4e52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the list request. There are several versions of the list request that only require email and key.\n",
    "#    This code sets the default action/requests to list the groups or parameter class descriptors. Having those descriptors \n",
    "#    allows one to request the individual (proprietary) 5 digit codes for individual air quality measures by using the\n",
    "#    param request. Some code in later cells will illustrate those requests.\n",
    "#\n",
    "def request_list_info(email_address = None, key = None,\n",
    "                      endpoint_url = API_REQUEST_URL, \n",
    "                      endpoint_action = API_ACTION_LIST_CLASSES, \n",
    "                      request_template = AQS_REQUEST_TEMPLATE,\n",
    "                      headers = None):\n",
    "    \n",
    "    #  Make sure we have email and key - at least\n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    \n",
    "    # For the basic request we need an email address and a key\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_list_info()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_list_info()'\")\n",
    "\n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a99082",
   "metadata": {},
   "source": [
    "According to the professor's example in [14], there are 7 pollutants that sensors may provide data on:\n",
    "- Carbon Monoxide\n",
    "- Sulfur dioxide\n",
    "- Nitrogen dioxide\n",
    "- Ozone\n",
    "- \"PM10 Total 0-10um STP\"\n",
    "- \"PM2.5 - Local Conditions\"\n",
    "- \"Acceptable PM2.5 AQI & Speciation Mass\"\n",
    "\n",
    "The natural question is which ones should we focus on? According to a guide on the AQI, Ozone and particle pollution are \"the major sources of unhealthy air quality around 99% of the time\" ([17] pg 18). Additionally, in an article on why wildfire pollutants are unsafe, the EPA says \"Particle pollution represents a main component of wildfire smoke and the principal public health threat\" [18]. Based on these reasons, I will ignore the gaseous pollutants: carbon monixide, sulfur dioxide, and nitrogen dioxide for my analysis, to simplify things.\n",
    "\n",
    "Therefore, reusing and modifying a cell from [14] we define a constant with the codes corresponding to the particle pollutants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8003147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Given the set of sensor codes, now we can create a parameter list or 'param' value as defined by the AQS API spec.\n",
    "#   It turns out that we want all of these measures for AQI, but we need to have two different param constants to get\n",
    "#   all seven of the code types. We can only have a max of 5 sensors/values request per param.\n",
    "#\n",
    "#   Gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "# AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "#\n",
    "#   Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\"\n",
    "#   \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a02073",
   "metadata": {},
   "source": [
    "**TBD: REDACT!!!!!!!!!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337dbfa",
   "metadata": {},
   "source": [
    "Now, I followed the steps in [14] to create an account for the API I will be using to get the AQI data. For the sake of privacy, I have redacted the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645d31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"obrienl@uw.edu\"\n",
    "APIKEY = \"bolefrog68\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758fe38",
   "metadata": {},
   "source": [
    "Now we determine which sensors are near Richland WA. Let's start with checking if there are any sensors in the county (Benton). According to [15] and [16], the FIPS code for Benton county is 53005 when you put together the state and county FIPS numbers. Note: I modified the two cells from [14] directly below to use Richland instead of two other city locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b929081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   We'll use these two city locations in the examples below.\n",
    "#\n",
    "CITY_LOCATIONS = {\n",
    "    'richland' :       {'city'   : 'Richland',\n",
    "                       'county' : 'Benton',\n",
    "                       'state'  : 'Washington',\n",
    "                       'fips'   : '53005',\n",
    "                       'latlon' : [46.28569070, -119.28446210] }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df89c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"0001\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0002\",\n",
      "        \"value_represented\": \"KENNEWICK - METALINE\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0003\",\n",
      "        \"value_represented\": \"Kennewick_S Clodfelter Rd\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0004\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"1001\",\n",
      "        \"value_represented\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  This list request should give us a list of all the monitoring stations in the county specified by the\n",
    "#  given city selected from the CITY_LOCATIONS dictionary\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = CITY_LOCATIONS['richland']['fips'][:2]   # the first two digits (characters) of FIPS is the state code\n",
    "request_data['county'] = CITY_LOCATIONS['richland']['fips'][2:]  # the last three digits (characters) of FIPS is the county code\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_SITES)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b06dd6",
   "metadata": {},
   "source": [
    "Looking at the data returned above, I am pleased to see that there are several monitoring stations within Benton County. The professor points out in the assingment instructions that even if there are monitoring stations within the same county as your city, it is unlikely that there will be any within the city limits. Thus, I will also use a \"geodetic bounding box\" to determine which monitoring stations are even closer to the city (assignment instructions), particularly the closest one. The following cell is copied from [14]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03fe654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   These are rough estimates for creating bounding boxes based on a city location\n",
    "#   You can find these rough estimates on the USGS website:\n",
    "#   https://www.usgs.gov/faqs/how-much-distance-does-a-degree-minute-and-second-cover-your-maps\n",
    "#\n",
    "LAT_25MILES = 25.0 * (1.0/69.0)    # This is about 25 miles of latitude in decimal degrees\n",
    "LON_25MILES = 25.0 * (1.0/54.6)    # This is about 25 miles of longitude in decimal degrees\n",
    "#\n",
    "#   Compute a rough estimates for a bounding box around a given place\n",
    "#   The bounding box is scaled in 50 mile increments. That is the bounding box will have sides that\n",
    "#   are rough multiples of 50 miles, with the center of the box around the indicated place.\n",
    "#   The scale parameter determines the scale (size) of the bounding box\n",
    "#\n",
    "def bounding_latlon(place=None,scale=1.0):\n",
    "    minlat = place['latlon'][0] - float(scale) * LAT_25MILES\n",
    "    maxlat = place['latlon'][0] + float(scale) * LAT_25MILES\n",
    "    minlon = place['latlon'][1] - float(scale) * LON_25MILES\n",
    "    maxlon = place['latlon'][1] + float(scale) * LON_25MILES\n",
    "    return [minlat,maxlat,minlon,maxlon]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87639352",
   "metadata": {},
   "source": [
    "After defining the bounding function above, we next resuse a cell from [14] to define a request function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b393b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the monitors request. This requests monitoring stations. This can be done by state, county, or bounding box. \n",
    "#\n",
    "#    Like the two other functions, this can be called with a mixture of a defined parameter dictionary, or with function\n",
    "#    parameters. If function parameters are provided, those take precedence over any parameters from the request template.\n",
    "#\n",
    "def request_monitors(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_MONITORS_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_monitors()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_monitors()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_monitors()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_monitors()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_monitors()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the monitors actions require the FIPS numbers\n",
    "    \n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a5b51",
   "metadata": {},
   "source": [
    "At this point, we can call the function using a geographic region. I modified a cell from [14] where the professor wrote code to do this. I tested various scale values and found that there is a single monitoring station within the region when the scale is set to $0.25$. It's address as seen below is: 5929 W METALINE (Kennewick Skills Center), Kennewick, WA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba0bc315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"state_code\": \"53\",\n",
      "        \"county_code\": \"005\",\n",
      "        \"site_number\": \"0002\",\n",
      "        \"parameter_code\": \"81102\",\n",
      "        \"poc\": 5,\n",
      "        \"parameter_name\": \"PM10 Total 0-10um STP\",\n",
      "        \"open_date\": \"2019-04-01\",\n",
      "        \"close_date\": null,\n",
      "        \"concurred_exclusions\": null,\n",
      "        \"dominant_source\": null,\n",
      "        \"measurement_scale\": \"NEIGHBORHOOD\",\n",
      "        \"measurement_scale_def\": \"500 M TO 4KM\",\n",
      "        \"monitoring_objective\": \"POPULATION EXPOSURE\",\n",
      "        \"last_method_code\": \"122\",\n",
      "        \"last_method_description\": \"INSTRUMENT MET ONE 4 MODELS - BETA ATTENUATION\",\n",
      "        \"last_method_begin_date\": \"2019-04-01\",\n",
      "        \"naaqs_primary_monitor\": \"Y\",\n",
      "        \"qa_primary_monitor\": null,\n",
      "        \"monitor_type\": \"SLAMS\",\n",
      "        \"networks\": null,\n",
      "        \"monitoring_agency_code\": \"1136\",\n",
      "        \"monitoring_agency\": \"Washington State Department Of Ecology\",\n",
      "        \"si_id\": 16457,\n",
      "        \"latitude\": 46.21835,\n",
      "        \"longitude\": -119.204153,\n",
      "        \"datum\": \"WGS84\",\n",
      "        \"lat_lon_accuracy\": 0.0,\n",
      "        \"elevation\": 162.0,\n",
      "        \"probe_height\": null,\n",
      "        \"pl_probe_location\": \"TOP OF BUILDING\",\n",
      "        \"local_site_name\": \"KENNEWICK - METALINE\",\n",
      "        \"address\": \"5929 W METALINE (Kennewick Skills Center)\",\n",
      "        \"state_name\": \"Washington\",\n",
      "        \"county_name\": \"Benton\",\n",
      "        \"city_name\": \"Kennewick\",\n",
      "        \"cbsa_code\": \"28420\",\n",
      "        \"cbsa_name\": \"Kennewick-Richland, WA\",\n",
      "        \"csa_code\": null,\n",
      "        \"csa_name\": null,\n",
      "        \"tribal_code\": null,\n",
      "        \"tribe_name\": null\n",
      "    },\n",
      "    {\n",
      "        \"state_code\": \"53\",\n",
      "        \"county_code\": \"005\",\n",
      "        \"site_number\": \"0002\",\n",
      "        \"parameter_code\": \"88502\",\n",
      "        \"poc\": 4,\n",
      "        \"parameter_name\": \"Acceptable PM2.5 AQI & Speciation Mass\",\n",
      "        \"open_date\": \"2005-10-19\",\n",
      "        \"close_date\": null,\n",
      "        \"concurred_exclusions\": null,\n",
      "        \"dominant_source\": null,\n",
      "        \"measurement_scale\": \"NEIGHBORHOOD\",\n",
      "        \"measurement_scale_def\": \"500 M TO 4KM\",\n",
      "        \"monitoring_objective\": \"POPULATION EXPOSURE\",\n",
      "        \"last_method_code\": \"771\",\n",
      "        \"last_method_description\": \"Correlated Radiance Research M903 With Heated Inlet - Nephelometry\",\n",
      "        \"last_method_begin_date\": \"2005-10-19\",\n",
      "        \"naaqs_primary_monitor\": null,\n",
      "        \"qa_primary_monitor\": null,\n",
      "        \"monitor_type\": \"SLAMS\",\n",
      "        \"networks\": null,\n",
      "        \"monitoring_agency_code\": \"1136\",\n",
      "        \"monitoring_agency\": \"Washington State Department Of Ecology\",\n",
      "        \"si_id\": 16457,\n",
      "        \"latitude\": 46.21835,\n",
      "        \"longitude\": -119.204153,\n",
      "        \"datum\": \"WGS84\",\n",
      "        \"lat_lon_accuracy\": 0.0,\n",
      "        \"elevation\": 162.0,\n",
      "        \"probe_height\": null,\n",
      "        \"pl_probe_location\": \"TOP OF BUILDING\",\n",
      "        \"local_site_name\": \"KENNEWICK - METALINE\",\n",
      "        \"address\": \"5929 W METALINE (Kennewick Skills Center)\",\n",
      "        \"state_name\": \"Washington\",\n",
      "        \"county_name\": \"Benton\",\n",
      "        \"city_name\": \"Kennewick\",\n",
      "        \"cbsa_code\": \"28420\",\n",
      "        \"cbsa_name\": \"Kennewick-Richland, WA\",\n",
      "        \"csa_code\": null,\n",
      "        \"csa_name\": null,\n",
      "        \"tribal_code\": null,\n",
      "        \"tribe_name\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#    Create a copy of the AQS_REQUEST_TEMPLATE\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['param'] = AQI_PARAMS_PARTICULATES     # same particulate request as the one abover\n",
    "# \n",
    "#   Not going to use these - comment them out\n",
    "#request_data['state'] = CITY_LOCATIONS['bend']['fips'][:2]\n",
    "#request_data['county'] = CITY_LOCATIONS['bend']['fips'][2:]\n",
    "#\n",
    "#   Now, we need bounding box parameters\n",
    "\n",
    "#------------------------LO--------------------------------\n",
    "bbox = bounding_latlon(CITY_LOCATIONS['richland'],scale=0.25)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "#   50 mile box\n",
    "# bbox = bounding_latlon(CITY_LOCATIONS['richland'],scale=1.0)\n",
    "#   100 mile box\n",
    "#bbox = bounding_latlon(CITY_LOCATIONS['bend'],scale=2.0)\n",
    "#   150 mile box\n",
    "#bbox = bounding_latlon(CITY_LOCATIONS['bend'],scale=3.0)\n",
    "#   200 mile box\n",
    "#bbox = bounding_latlon(CITY_LOCATIONS['bend'],scale=4.0)\n",
    "\n",
    "# the bbox response comes back as a list - [minlat,maxlat,minlon,maxlon]\n",
    "\n",
    "#   put our bounding box into the request_data\n",
    "request_data['minlat'] = bbox[0]\n",
    "request_data['maxlat'] = bbox[1]\n",
    "request_data['minlon'] = bbox[2]\n",
    "request_data['maxlon'] = bbox[3]\n",
    "\n",
    "#\n",
    "#   we need to change the action for the API from the default to the bounding box - same recent date for now\n",
    "response = request_monitors(request_template=request_data, begin_date=\"20210701\", end_date=\"20210731\",\n",
    "                            endpoint_action = API_ACTION_MONITORS_BOX)\n",
    "#\n",
    "#\n",
    "#\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b26fc",
   "metadata": {},
   "source": [
    "Now, let's expand the search to the sensor locations within 50 miles of Richland to see more results. We again use the code from [14] in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b78612f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"state_code\": \"53\",\n",
      "        \"county_code\": \"005\",\n",
      "        \"site_number\": \"0002\",\n",
      "        \"parameter_code\": \"81102\",\n",
      "        \"poc\": 5,\n",
      "        \"parameter_name\": \"PM10 Total 0-10um STP\",\n",
      "        \"open_date\": \"2019-04-01\",\n",
      "        \"close_date\": null,\n",
      "        \"concurred_exclusions\": null,\n",
      "        \"dominant_source\": null,\n",
      "        \"measurement_scale\": \"NEIGHBORHOOD\",\n",
      "        \"measurement_scale_def\": \"500 M TO 4KM\",\n",
      "        \"monitoring_objective\": \"POPULATION EXPOSURE\",\n",
      "        \"last_method_code\": \"122\",\n",
      "        \"last_method_description\": \"INSTRUMENT MET ONE 4 MODELS - BETA ATTENUATION\",\n",
      "        \"last_method_begin_date\": \"2019-04-01\",\n",
      "        \"naaqs_primary_monitor\": \"Y\",\n",
      "        \"qa_primary_monitor\": null,\n",
      "        \"monitor_type\": \"SLAMS\",\n",
      "        \"networks\": null,\n",
      "        \"monitoring_agency_code\": \"1136\",\n",
      "        \"monitoring_agency\": \"Washington State Department Of Ecology\",\n",
      "        \"si_id\": 16457,\n",
      "        \"latitude\": 46.21835,\n",
      "        \"longitude\": -119.204153,\n",
      "        \"datum\": \"WGS84\",\n",
      "        \"lat_lon_accuracy\": 0.0,\n",
      "        \"elevation\": 162.0,\n",
      "        \"probe_height\": null,\n",
      "        \"pl_probe_location\": \"TOP OF BUILDING\",\n",
      "        \"local_site_name\": \"KENNEWICK - METALINE\",\n",
      "        \"address\": \"5929 W METALINE (Kennewick Skills Center)\",\n",
      "        \"state_name\": \"Washington\",\n",
      "        \"county_name\": \"Benton\",\n",
      "        \"city_name\": \"Kennewick\",\n",
      "        \"cbsa_code\": \"28420\",\n",
      "        \"cbsa_name\": \"Kennewick-Richland, WA\",\n",
      "        \"csa_code\": null,\n",
      "        \"csa_name\": null,\n",
      "        \"tribal_code\": null,\n",
      "        \"tribe_name\": null\n",
      "    },\n",
      "    {\n",
      "        \"state_code\": \"53\",\n",
      "        \"county_code\": \"071\",\n",
      "        \"site_number\": \"0006\",\n",
      "        \"parameter_code\": \"81102\",\n",
      "        \"poc\": 5,\n",
      "        \"parameter_name\": \"PM10 Total 0-10um STP\",\n",
      "        \"open_date\": \"2019-10-03\",\n",
      "        \"close_date\": null,\n",
      "        \"concurred_exclusions\": null,\n",
      "        \"dominant_source\": null,\n",
      "        \"measurement_scale\": \"NEIGHBORHOOD\",\n",
      "        \"measurement_scale_def\": \"500 M TO 4KM\",\n",
      "        \"monitoring_objective\": \"POPULATION EXPOSURE\",\n",
      "        \"last_method_code\": \"122\",\n",
      "        \"last_method_description\": \"INSTRUMENT MET ONE 4 MODELS - BETA ATTENUATION\",\n",
      "        \"last_method_begin_date\": \"2019-10-03\",\n",
      "        \"naaqs_primary_monitor\": null,\n",
      "        \"qa_primary_monitor\": null,\n",
      "        \"monitor_type\": \"SLAMS\",\n",
      "        \"networks\": null,\n",
      "        \"monitoring_agency_code\": \"1136\",\n",
      "        \"monitoring_agency\": \"Washington State Department Of Ecology\",\n",
      "        \"si_id\": 91449,\n",
      "        \"latitude\": 46.199901,\n",
      "        \"longitude\": -119.008329,\n",
      "        \"datum\": \"WGS84\",\n",
      "        \"lat_lon_accuracy\": 5.0,\n",
      "        \"elevation\": 179.0,\n",
      "        \"probe_height\": null,\n",
      "        \"pl_probe_location\": null,\n",
      "        \"local_site_name\": \"BURBANK - MAPLE ST\",\n",
      "        \"address\": \"755 MAPLE STREET (Columbia High School)\",\n",
      "        \"state_name\": \"Washington\",\n",
      "        \"county_name\": \"Walla Walla\",\n",
      "        \"city_name\": \"Burbank\",\n",
      "        \"cbsa_code\": \"47460\",\n",
      "        \"cbsa_name\": \"Walla Walla, WA\",\n",
      "        \"csa_code\": null,\n",
      "        \"csa_name\": null,\n",
      "        \"tribal_code\": null,\n",
      "        \"tribe_name\": null\n",
      "    },\n",
      "    {\n",
      "        \"state_code\": \"53\",\n",
      "        \"county_code\": \"021\",\n",
      "        \"site_number\": \"0002\",\n",
      "        \"parameter_code\": \"88502\",\n",
      "        \"poc\": 4,\n",
      "        \"parameter_name\": \"Acceptable PM2.5 AQI & Speciation Mass\",\n",
      "        \"open_date\": \"2003-01-15\",\n",
      "        \"close_date\": null,\n",
      "        \"concurred_exclusions\": null,\n",
      "        \"dominant_source\": null,\n",
      "        \"measurement_scale\": \"NEIGHBORHOOD\",\n",
      "        \"measurement_scale_def\": \"500 M TO 4KM\",\n",
      "        \"monitoring_objective\": \"POPULATION EXPOSURE\",\n",
      "        \"last_method_code\": \"771\",\n",
      "        \"last_method_description\": \"Correlated Radiance Research M903 With Heated Inlet - Nephelometry\",\n",
      "        \"last_method_begin_date\": \"2003-01-15\",\n",
      "        \"naaqs_primary_monitor\": null,\n",
      "        \"qa_primary_monitor\": null,\n",
      "        \"monitor_type\": \"SLAMS\",\n",
      "        \"networks\": null,\n",
      "        \"monitoring_agency_code\": \"1136\",\n",
      "        \"monitoring_agency\": \"Washington State Department Of Ecology\",\n",
      "        \"si_id\": 91523,\n",
      "        \"latitude\": 46.5754,\n",
      "        \"longitude\": -119.0021,\n",
      "        \"datum\": \"WGS84\",\n",
      "        \"lat_lon_accuracy\": 5.0,\n",
      "        \"elevation\": 224.0,\n",
      "        \"probe_height\": 6.0,\n",
      "        \"pl_probe_location\": \"TOP OF BUILDING\",\n",
      "        \"local_site_name\": \"MESA - PEPOIT WAY\",\n",
      "        \"address\": \"200 PEPIOT WAY (Mesa Elementary School)\",\n",
      "        \"state_name\": \"Washington\",\n",
      "        \"county_name\": \"Franklin\",\n",
      "        \"city_name\": \"Mesa\",\n",
      "        \"cbsa_code\": \"28420\",\n",
      "        \"cbsa_name\": \"Kennewick-Richland, WA\",\n",
      "        \"csa_code\": null,\n",
      "        \"csa_name\": null,\n",
      "        \"tribal_code\": null,\n",
      "        \"tribe_name\": null\n",
      "    },\n",
      "    {\n",
      "        \"state_code\": \"53\",\n",
      "        \"county_code\": \"005\",\n",
      "        \"site_number\": \"0002\",\n",
      "        \"parameter_code\": \"88502\",\n",
      "        \"poc\": 4,\n",
      "        \"parameter_name\": \"Acceptable PM2.5 AQI & Speciation Mass\",\n",
      "        \"open_date\": \"2005-10-19\",\n",
      "        \"close_date\": null,\n",
      "        \"concurred_exclusions\": null,\n",
      "        \"dominant_source\": null,\n",
      "        \"measurement_scale\": \"NEIGHBORHOOD\",\n",
      "        \"measurement_scale_def\": \"500 M TO 4KM\",\n",
      "        \"monitoring_objective\": \"POPULATION EXPOSURE\",\n",
      "        \"last_method_code\": \"771\",\n",
      "        \"last_method_description\": \"Correlated Radiance Research M903 With Heated Inlet - Nephelometry\",\n",
      "        \"last_method_begin_date\": \"2005-10-19\",\n",
      "        \"naaqs_primary_monitor\": null,\n",
      "        \"qa_primary_monitor\": null,\n",
      "        \"monitor_type\": \"SLAMS\",\n",
      "        \"networks\": null,\n",
      "        \"monitoring_agency_code\": \"1136\",\n",
      "        \"monitoring_agency\": \"Washington State Department Of Ecology\",\n",
      "        \"si_id\": 16457,\n",
      "        \"latitude\": 46.21835,\n",
      "        \"longitude\": -119.204153,\n",
      "        \"datum\": \"WGS84\",\n",
      "        \"lat_lon_accuracy\": 0.0,\n",
      "        \"elevation\": 162.0,\n",
      "        \"probe_height\": null,\n",
      "        \"pl_probe_location\": \"TOP OF BUILDING\",\n",
      "        \"local_site_name\": \"KENNEWICK - METALINE\",\n",
      "        \"address\": \"5929 W METALINE (Kennewick Skills Center)\",\n",
      "        \"state_name\": \"Washington\",\n",
      "        \"county_name\": \"Benton\",\n",
      "        \"city_name\": \"Kennewick\",\n",
      "        \"cbsa_code\": \"28420\",\n",
      "        \"cbsa_name\": \"Kennewick-Richland, WA\",\n",
      "        \"csa_code\": null,\n",
      "        \"csa_name\": null,\n",
      "        \"tribal_code\": null,\n",
      "        \"tribe_name\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#    Create a copy of the AQS_REQUEST_TEMPLATE\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['param'] = AQI_PARAMS_PARTICULATES     # same particulate request as the one abover\n",
    "# \n",
    "#   Not going to use these - comment them out\n",
    "#request_data['state'] = CITY_LOCATIONS['bend']['fips'][:2]\n",
    "#request_data['county'] = CITY_LOCATIONS['bend']['fips'][2:]\n",
    "#\n",
    "#   Now, we need bounding box parameters\n",
    "\n",
    "#------------------------LO--------------------------------\n",
    "#   50 mile box\n",
    "# bbox = bounding_latlon(CITY_LOCATIONS['richland'],scale=0.25)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "#   50 mile box\n",
    "bbox = bounding_latlon(CITY_LOCATIONS['richland'],scale=1.0)\n",
    "#   100 mile box\n",
    "#bbox = bounding_latlon(CITY_LOCATIONS['bend'],scale=2.0)\n",
    "#   150 mile box\n",
    "#bbox = bounding_latlon(CITY_LOCATIONS['bend'],scale=3.0)\n",
    "#   200 mile box\n",
    "#bbox = bounding_latlon(CITY_LOCATIONS['bend'],scale=4.0)\n",
    "\n",
    "# the bbox response comes back as a list - [minlat,maxlat,minlon,maxlon]\n",
    "\n",
    "#   put our bounding box into the request_data\n",
    "request_data['minlat'] = bbox[0]\n",
    "request_data['maxlat'] = bbox[1]\n",
    "request_data['minlon'] = bbox[2]\n",
    "request_data['maxlon'] = bbox[3]\n",
    "\n",
    "#\n",
    "#   we need to change the action for the API from the default to the bounding box - same recent date for now\n",
    "response = request_monitors(request_template=request_data, begin_date=\"20210701\", end_date=\"20210731\",\n",
    "                            endpoint_action = API_ACTION_MONITORS_BOX)\n",
    "#\n",
    "#\n",
    "#\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12214e3",
   "metadata": {},
   "source": [
    "**TBD Cleanup text below!!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb877333",
   "metadata": {},
   "source": [
    "**how do I use this result? Should I focus on a single monitor or combine results??????**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920b5f7",
   "metadata": {},
   "source": [
    "What do I know:\n",
    "- The monitor stations in the same county\n",
    "- The closest monitor station and if I do the math, it's distance from Richland\n",
    "- There are 3 types of particulate a sensor can measure. \n",
    "- There are three sites with 50 miles of Richland. Unfortuneately, the earliest was not established until 2003. \n",
    "\n",
    "By running the cell above, we see the sensors appearing within 50 miles of Richland and the pollutants that they provide. I summarized the results as follows:\n",
    "- Site 0002: PM10 Total 0-10um STP, open_date 2019-04-01 --> 5929 W METALINE, Kennewick\n",
    "- Site 0006: PM10 Total 0-10um STP, open_date 2019-10-03 --> 755 MAPLE STREET, Burbank\n",
    "- Site 0002: Acceptable PM2.5 AQI & Speciation Mass, open_date 2003-01-15 --> 200 PEPIOT WAY, Mesa\n",
    "- Site 0002: Acceptable PM2.5 AQI & Speciation Mass, open_date 2005-10-19 --> 5929 W METALINE, Kenewick\n",
    "\n",
    "I do not know the definitions of several of the terms provided in the results. In particular, I assume that the site number is a unique identifier for each location, however, this is contradicted by the fact that Site 0002 is linked to a site in Kennewick and a site in Mesa. Regardless, looking at the types of pollutant info provided, we see the two kinds are \"PM10 Total 0-10um STP\" and \"Acceptable PM2.5 AQI & Speciation Mass\". Since both of these are provided by the Kennewick 0002 site, I will solely reference that site for the AQI for my analysis, as it is the closest one to Richland - however, I do note that I am sacrificing 2 years of PM2.5 data as the Kennewick location started providing this over two years later than the Mesa 0006 site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba9305",
   "metadata": {},
   "source": [
    "Armed with the knowledge of which sensor we will use and which pollutant types it provides data on, we proceed by writing a new API request function to get the daily values of AQI. The code in the cell below resuses a cell from [14] verbatim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4406ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the daily summary request. Daily summary provides a daily summary value for each sensor being requested\n",
    "#    from the start date to the end date. \n",
    "#\n",
    "#    Like the two other functions, this can be called with a mixture of a defined parameter dictionary, or with function\n",
    "#    parameters. If function parameters are provided, those take precedence over any parameters from the request template.\n",
    "#\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the daily summary actions require the FIPS numbers\n",
    "        \n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3423fa",
   "metadata": {},
   "source": [
    "Next, we tidy up the output by defining a summary function. The code cell below is directly copied from [14]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3088b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This is a list of field names - data - that will be extracted from each record\n",
    "#\n",
    "EXTRACTION_FIELDS = ['sample_duration','observation_count','arithmetic_mean','aqi']\n",
    "\n",
    "#\n",
    "#    The function creates a summary record\n",
    "def extract_summary_from_response(r=None, fields=EXTRACTION_FIELDS):\n",
    "    ## the result will be structured around monitoring site, parameter, and then date\n",
    "    result = dict()\n",
    "    data = r[\"Data\"]\n",
    "    for record in data:\n",
    "        # make sure the record is set up\n",
    "        site = record['site_number']\n",
    "        param = record['parameter_code']\n",
    "        #date = record['date_local']    # this version keeps the respnse value YYYY-\n",
    "        date = record['date_local'].replace('-','') # this puts it in YYYYMMDD format\n",
    "        if site not in result:\n",
    "            result[site] = dict()\n",
    "            result[site]['local_site_name'] = record['local_site_name']\n",
    "            result[site]['site_address'] = record['site_address']\n",
    "            result[site]['state'] = record['state']\n",
    "            result[site]['county'] = record['county']\n",
    "            result[site]['city'] = record['city']\n",
    "            result[site]['pollutant_type'] = dict()\n",
    "        if param not in result[site]['pollutant_type']:\n",
    "            result[site]['pollutant_type'][param] = dict()\n",
    "            result[site]['pollutant_type'][param]['parameter_name'] = record['parameter']\n",
    "            result[site]['pollutant_type'][param]['units_of_measure'] = record['units_of_measure']\n",
    "            result[site]['pollutant_type'][param]['method'] = record['method']\n",
    "            result[site]['pollutant_type'][param]['data'] = dict()\n",
    "        if date not in result[site]['pollutant_type'][param]['data']:\n",
    "            result[site]['pollutant_type'][param]['data'][date] = list()\n",
    "        \n",
    "        # now extract the specified fields\n",
    "        extract = dict()\n",
    "        for k in fields:\n",
    "            if str(k) in record:\n",
    "                extract[str(k)] = record[k]\n",
    "            else:\n",
    "                # this makes sure we always have the requested fields, even if\n",
    "                # we have a missing value for a given day/month\n",
    "                extract[str(k)] = None\n",
    "        \n",
    "        # add this extraction to the list for the day\n",
    "        result[site]['pollutant_type'][param]['data'][date].append(extract)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bd22c",
   "metadata": {},
   "source": [
    "Next we start the proecess for requesting the daily summary data. The cell below is reused and modified from [14]. Some of the main changes I made to the cell are that:\n",
    "- I implemented the ability to loop through multiple years at once\n",
    "- I removed the request for gaseous AQI\n",
    "- Commented out passing state and county arguments to try to operate by bounding box instead to get a specific site\n",
    "- Added code to pass the latlongs for the request template\n",
    "- Changed the endpoint_action to use a geographic box instead of a county.\n",
    "- TBD: pass the 4 latlong parameters to request_data\n",
    "- TBD: update particulate codes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ec61e",
   "metadata": {},
   "source": [
    "**TBD Should I update which particulates I am grabbing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cacf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d199b34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for the particulate pollutants in 2022...\n",
      "Success\n",
      "Response for the particulate pollutants in 2023...\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "from datetime import date \n",
    "\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "# request_data['state'] = CITY_LOCATIONS['richland']['fips'][:2]\n",
    "# request_data['county'] = CITY_LOCATIONS['richland']['fips'][2:]\n",
    "\n",
    "bbox = bounding_latlon(CITY_LOCATIONS['richland'],scale=0.25)\n",
    "\n",
    "#   put our bounding box into the request_data\n",
    "request_data['minlat'] = bbox[0]\n",
    "request_data['maxlat'] = bbox[1]\n",
    "request_data['minlon'] = bbox[2]\n",
    "request_data['maxlon'] = bbox[3]\n",
    "\n",
    "\n",
    "# request daily summary data for all years\n",
    "# START_DATE = '20220101' #should be 1963\n",
    "# PRESENT_DATE = '20231108'\n",
    "\n",
    "START_DATE = '19630101' #should be 1963\n",
    "PRESENT_DATE = '20231108'\n",
    "\n",
    "# get every year between both dates\n",
    "# I learned to do this from: https://stackoverflow.com/questions/23590042/get-all-years-between-two-dates-in-python\n",
    "start_date = datetime.strptime(START_DATE, '%Y%m%d')\n",
    "end_date = datetime.strptime(PRESENT_DATE, '%Y%m%d')\n",
    "\n",
    "\n",
    "# Retrieve the AQI data for each year\n",
    "particulate_aqi_ls = []\n",
    "year_range = range(start_date.year, end_date.year + 1)\n",
    "for year in year_range:\n",
    "    \n",
    "    # determine current start and end dates\n",
    "    # I referenced: https://www.geeksforgeeks.org/add-years-to-datetime-object-in-python/\n",
    "    # and: https://stackoverflow.com/questions/15741618/add-one-year-in-current-date-python\n",
    "    current_start_date = date(year, 1, 1)\n",
    "    \n",
    "    if year == end_date.year: # if you've reached final year you won't grab entire year\n",
    "        current_end_date = end_date\n",
    "    \n",
    "    else: # otherwise your current end date is the last day of December of current year\n",
    "        current_end_date = date(year, 12, 31)\n",
    "    \n",
    "    # get the dates in string format\n",
    "    # referenced: https://www.programiz.com/python-programming/datetime/strftime\n",
    "    current_start_date_str = current_start_date.strftime('%Y%m%d')\n",
    "    current_end_date_str = current_end_date.strftime('%Y%m%d')\n",
    "    \n",
    "    # call the api -------------------------------------------------------------------------------------------------------------\n",
    "    particulate_aqi = request_daily_summary(request_template=request_data, \n",
    "                                            begin_date=current_start_date_str, end_date=current_end_date_str, \n",
    "                                            endpoint_action = API_ACTION_DAILY_SUMMARY_BOX)\n",
    "    # check that it worked\n",
    "    print(\"Response for the particulate pollutants in {}...\".format(year))\n",
    "    #\n",
    "    if particulate_aqi[\"Header\"][0]['status'] == \"Success\":\n",
    "    #     print(json.dumps(particulate_aqi['Data'],indent=4))\n",
    "        print('Success')\n",
    "    elif particulate_aqi[\"Header\"][0]['status'].startswith(\"No data \"):\n",
    "        print(\"Looks like the response generated no data. You might take a closer look at your request and the response data.\")\n",
    "        break\n",
    "    else:\n",
    "        print(json.dumps(particulate_aqi,indent=4))\n",
    "        break\n",
    "    #     print('No idea what is going on')\n",
    "    \n",
    "    # tidy up output\n",
    "    extract_particulate_dict = extract_summary_from_response(particulate_aqi)\n",
    "    \n",
    "    #store data\n",
    "    particulate_aqi_ls.append(extract_particulate_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bb983",
   "metadata": {},
   "source": [
    "Call the extract summary function to tidy up the results. The cell below is reused and modified from [14]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08f9bb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract_particulate = extract_summary_from_response(particulate_aqi)\n",
    "# print(\"Summary of particulate extraction ...\")\n",
    "# print(json.dumps(extract_particulate,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db78259",
   "metadata": {},
   "source": [
    "Let's recap where we are at at this point. We are seeking to retrieve a value for AQI from the EPA API that we can compare to our smoke estimates. We have decided based on our exploration to get data from the Site 0002 in Kennewick Washington, another town in Benton county. Additionally, we have demonstrated that we can get the daily summary data for this site for a specified date range. \n",
    "\n",
    "Now, since we are ultimately going to chart the smoke estimate and AQI at the annual level, we need to figure out a way to move from a daily granularity AQI for two provided particle types, to a single AQI value for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855443e",
   "metadata": {},
   "source": [
    "So, first we store the output of the extract summary as a data type that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4869aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_particulate_dict = extract_summary_from_response(particulate_aqi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310156cd",
   "metadata": {},
   "source": [
    "Now, let's retrive the information we need and make this easier to work with by storing the aqi in a couple dictionaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9817767",
   "metadata": {},
   "source": [
    "**TBD Decision, right now, the next cell allows AQI to be None if necessary. should I change to a constant like '-1'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f1a86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 6492.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Retrieving data for 2022 ------------\n",
      "Retrieving pm10 AQI data\n",
      "AQI for pm10 data with date 20221013 is null\n",
      "AQI for pm10 data with date 20221017 is null\n",
      "There were 2 instances of missing data.\n",
      "There were 0 null values in the data.\n",
      "Total null values retrived: 2\n",
      "\n",
      "Retrieving pm2.5 Acceptable AQI data\n",
      "AQI for pm2.5 acceptable data with date 20220420 is null\n",
      "There were 1 instances of missing data.\n",
      "There were 0 null values in the data.\n",
      "Total null values retrived: 1\n",
      "------------ Retrieving data for 2023 ------------\n",
      "Retrieving pm10 AQI data\n",
      "AQI for pm10 data with date 20230313 is null\n",
      "AQI for pm10 data with date 20230314 is null\n",
      "AQI for pm10 data with date 20230614 is null\n",
      "AQI for pm10 data with date 20230621 is null\n",
      "There were 4 instances of missing data.\n",
      "There were 0 null values in the data.\n",
      "Total null values retrived: 4\n",
      "\n",
      "Retrieving pm2.5 Acceptable AQI data\n",
      "There were 0 instances of missing data.\n",
      "There were 0 null values in the data.\n",
      "Total null values retrived: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reference:https://www.geeksforgeeks.org/range-to-a-list-in-python/\n",
    "year_list = [*year_range]\n",
    "year_index = 0\n",
    "\n",
    "# define my dictionaries outside the loop so that the hold values for all years\n",
    "pm10_aqi_dict = {}\n",
    "pm25_aqi_dict = {}\n",
    "\n",
    "# iterate through all years of data\n",
    "for extract_particulate_dict in tqdm(particulate_aqi_ls): # for each year\n",
    "    year = year_list[year_index]\n",
    "    print(\"------------ Retrieving data for {} ------------\".format(year))\n",
    "    site_data = extract_particulate_dict['0002']\n",
    "\n",
    "    # get the data for each pollutant type\n",
    "    pm10_data_dict = site_data['pollutant_type']['81102']['data']\n",
    "    acceptable_pm2_5_data_dict = site_data['pollutant_type']['88502']['data']\n",
    "\n",
    "    # iterate over the dictionaries\n",
    "    # I viewed https://www.geeksforgeeks.org/iterate-over-a-dictionary-in-python/\n",
    "    # to learn how to iterate over a dictionary\n",
    "\n",
    "    # first we get lists of the keys\n",
    "    pm10_data_keys = pm10_data_dict.keys()\n",
    "    acceptable_pm2_5_data_keys = acceptable_pm2_5_data_dict.keys()\n",
    "\n",
    "    # iterate through the dictionary of dates for pm10 and grab the aqi for each\n",
    "    print(\"Retrieving pm10 AQI data\")\n",
    "    pm10_missing_count = 0\n",
    "    pm10_null_count = 0\n",
    "    for key in pm10_data_keys:\n",
    "\n",
    "        try:\n",
    "            aqi = pm10_data_dict[key][1]['aqi']\n",
    "\n",
    "        except:\n",
    "            aqi = None\n",
    "            pm10_missing_count = pm10_missing_count + 1\n",
    "            print(\"AQI for pm10 data with date {} is null\".format(key))\n",
    "\n",
    "        #flag potential issues\n",
    "        else:\n",
    "            if aqi is None:\n",
    "                print(\"AQI for pm10 data with date {} is null\".format(key))\n",
    "                pm10_null_count = pm10_null_count + 1\n",
    "\n",
    "        pm10_aqi_dict[key] = aqi\n",
    "\n",
    "    print(\"There were {} instances of missing data.\".format(pm10_missing_count)) #where no 24 hr data\n",
    "    print(\"There were {} null values in the data.\".format(pm10_null_count))\n",
    "    print(\"Total null values retrived: {}\".format(pm10_missing_count + pm10_null_count))\n",
    "\n",
    "    # iterate through the dictionary of dates for pm2.5 acceptability and grab the aqi for each\n",
    "    print('')\n",
    "    print(\"Retrieving pm2.5 Acceptable AQI data\")\n",
    "    pm25_missing_count = 0\n",
    "    pm25_nulls_count = 0\n",
    "    for key in acceptable_pm2_5_data_keys:\n",
    "\n",
    "        try:\n",
    "            aqi = acceptable_pm2_5_data_dict[key][1]['aqi']\n",
    "\n",
    "        except:\n",
    "            aqi = None\n",
    "            pm25_missing_count = pm25_missing_count + 1\n",
    "            print(\"AQI for pm2.5 acceptable data with date {} is null\".format(key))\n",
    "\n",
    "        #flag potential issues\n",
    "        else:\n",
    "            if aqi is None:\n",
    "                print(\"AQI for pm2.5 acceptable data with date {} is null\".format(key))\n",
    "                pm25_nulls_count = pm25_nulls_count + 1\n",
    "\n",
    "        pm25_aqi_dict[key] = aqi\n",
    "\n",
    "    print(\"There were {} instances of missing data.\".format(pm25_missing_count)) #where no 24 hr data\n",
    "    print(\"There were {} null values in the data.\".format(pm25_nulls_count))\n",
    "    print(\"Total null values retrived: {}\".format(pm25_missing_count + pm25_nulls_count))\n",
    "    \n",
    "    #update year index\n",
    "    year_index = year_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b669545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20220101': 14,\n",
       " '20220102': 12,\n",
       " '20220103': 10,\n",
       " '20220104': 9,\n",
       " '20220105': 8,\n",
       " '20220106': 6,\n",
       " '20220107': 5,\n",
       " '20220108': 6,\n",
       " '20220109': 6,\n",
       " '20220110': 14,\n",
       " '20220111': 19,\n",
       " '20220112': 20,\n",
       " '20220113': 19,\n",
       " '20220114': 12,\n",
       " '20220115': 6,\n",
       " '20220116': 5,\n",
       " '20220117': 14,\n",
       " '20220118': 6,\n",
       " '20220119': 8,\n",
       " '20220120': 7,\n",
       " '20220121': 6,\n",
       " '20220122': 8,\n",
       " '20220123': 6,\n",
       " '20220124': 8,\n",
       " '20220125': 12,\n",
       " '20220126': 7,\n",
       " '20220127': 6,\n",
       " '20220128': 10,\n",
       " '20220129': 15,\n",
       " '20220130': 15,\n",
       " '20220131': 6,\n",
       " '20220201': 6,\n",
       " '20220202': 11,\n",
       " '20220203': 12,\n",
       " '20220204': 17,\n",
       " '20220205': 7,\n",
       " '20220206': 15,\n",
       " '20220207': 20,\n",
       " '20220208': 7,\n",
       " '20220209': 18,\n",
       " '20220210': 13,\n",
       " '20220211': 13,\n",
       " '20220212': 9,\n",
       " '20220213': 12,\n",
       " '20220214': 17,\n",
       " '20220215': 7,\n",
       " '20220216': 8,\n",
       " '20220217': 14,\n",
       " '20220218': 10,\n",
       " '20220219': 23,\n",
       " '20220220': 4,\n",
       " '20220221': 16,\n",
       " '20220222': 21,\n",
       " '20220223': 8,\n",
       " '20220224': 7,\n",
       " '20220225': 13,\n",
       " '20220226': 13,\n",
       " '20220227': 16,\n",
       " '20220228': 8,\n",
       " '20220301': 5,\n",
       " '20220302': 6,\n",
       " '20220303': 6,\n",
       " '20220304': 6,\n",
       " '20220305': 6,\n",
       " '20220306': 7,\n",
       " '20220307': 12,\n",
       " '20220308': 6,\n",
       " '20220309': 7,\n",
       " '20220310': 10,\n",
       " '20220311': 21,\n",
       " '20220312': 18,\n",
       " '20220313': 18,\n",
       " '20220314': 8,\n",
       " '20220315': 17,\n",
       " '20220316': 6,\n",
       " '20220317': 11,\n",
       " '20220318': 7,\n",
       " '20220319': 10,\n",
       " '20220320': 4,\n",
       " '20220321': 6,\n",
       " '20220322': 9,\n",
       " '20220323': 14,\n",
       " '20220324': 9,\n",
       " '20220325': 14,\n",
       " '20220326': 19,\n",
       " '20220327': 17,\n",
       " '20220328': 18,\n",
       " '20220329': 10,\n",
       " '20220330': 19,\n",
       " '20220331': 7,\n",
       " '20220401': 13,\n",
       " '20220402': 6,\n",
       " '20220403': 9,\n",
       " '20220404': 100,\n",
       " '20220405': 16,\n",
       " '20220406': 11,\n",
       " '20220407': 18,\n",
       " '20220408': 44,\n",
       " '20220409': 7,\n",
       " '20220410': 6,\n",
       " '20220411': 3,\n",
       " '20220412': 5,\n",
       " '20220413': 7,\n",
       " '20220414': 8,\n",
       " '20220415': 6,\n",
       " '20220416': 4,\n",
       " '20220417': 3,\n",
       " '20220418': 6,\n",
       " '20220419': 5,\n",
       " '20220420': 6,\n",
       " '20220421': 6,\n",
       " '20220422': 6,\n",
       " '20220423': 5,\n",
       " '20220424': 9,\n",
       " '20220425': 12,\n",
       " '20220426': 13,\n",
       " '20220427': 7,\n",
       " '20220428': 10,\n",
       " '20220429': 8,\n",
       " '20220430': 6,\n",
       " '20220501': 4,\n",
       " '20220502': 4,\n",
       " '20220503': 6,\n",
       " '20220504': 8,\n",
       " '20220505': 7,\n",
       " '20220506': 4,\n",
       " '20220507': 9,\n",
       " '20220508': 3,\n",
       " '20220509': 6,\n",
       " '20220510': 6,\n",
       " '20220511': 8,\n",
       " '20220512': 11,\n",
       " '20220513': 6,\n",
       " '20220514': 6,\n",
       " '20220515': 6,\n",
       " '20220516': 10,\n",
       " '20220517': 8,\n",
       " '20220518': 38,\n",
       " '20220519': 9,\n",
       " '20220520': 9,\n",
       " '20220521': 7,\n",
       " '20220522': 7,\n",
       " '20220523': 13,\n",
       " '20220524': 13,\n",
       " '20220525': 10,\n",
       " '20220526': 16,\n",
       " '20220527': 9,\n",
       " '20220528': 8,\n",
       " '20220529': 11,\n",
       " '20220530': 6,\n",
       " '20220531': 10,\n",
       " '20220601': 18,\n",
       " '20220602': 19,\n",
       " '20220603': 10,\n",
       " '20220604': 3,\n",
       " '20220605': 12,\n",
       " '20220606': 7,\n",
       " '20220607': 10,\n",
       " '20220608': 13,\n",
       " '20220609': 15,\n",
       " '20220610': 6,\n",
       " '20220611': 8,\n",
       " '20220612': 5,\n",
       " '20220613': 22,\n",
       " '20220614': 13,\n",
       " '20220615': 10,\n",
       " '20220616': 13,\n",
       " '20220617': 15,\n",
       " '20220618': 6,\n",
       " '20220619': 6,\n",
       " '20220620': 8,\n",
       " '20220621': 12,\n",
       " '20220622': 19,\n",
       " '20220623': 14,\n",
       " '20220624': 13,\n",
       " '20220625': 13,\n",
       " '20220626': 15,\n",
       " '20220627': 25,\n",
       " '20220628': 58,\n",
       " '20220629': 13,\n",
       " '20220630': 14,\n",
       " '20220701': 19,\n",
       " '20220702': 17,\n",
       " '20220703': 14,\n",
       " '20220704': 12,\n",
       " '20220705': 13,\n",
       " '20220706': 11,\n",
       " '20220707': 13,\n",
       " '20220708': 12,\n",
       " '20220709': 12,\n",
       " '20220710': 11,\n",
       " '20220711': 16,\n",
       " '20220712': 23,\n",
       " '20220713': 19,\n",
       " '20220714': 18,\n",
       " '20220715': 26,\n",
       " '20220716': 14,\n",
       " '20220717': 23,\n",
       " '20220718': 17,\n",
       " '20220719': 21,\n",
       " '20220720': 22,\n",
       " '20220721': 21,\n",
       " '20220722': 25,\n",
       " '20220723': 16,\n",
       " '20220724': 16,\n",
       " '20220725': 25,\n",
       " '20220726': 28,\n",
       " '20220727': 34,\n",
       " '20220728': 34,\n",
       " '20220729': 33,\n",
       " '20220730': 28,\n",
       " '20220731': 22,\n",
       " '20220801': 33,\n",
       " '20220802': 29,\n",
       " '20220803': 28,\n",
       " '20220804': 29,\n",
       " '20220805': 18,\n",
       " '20220806': 19,\n",
       " '20220807': 19,\n",
       " '20220808': 29,\n",
       " '20220809': 54,\n",
       " '20220810': 32,\n",
       " '20220811': 24,\n",
       " '20220812': 31,\n",
       " '20220813': 24,\n",
       " '20220814': 13,\n",
       " '20220815': 22,\n",
       " '20220816': 26,\n",
       " '20220817': 38,\n",
       " '20220818': 35,\n",
       " '20220819': 44,\n",
       " '20220820': 34,\n",
       " '20220821': 17,\n",
       " '20220822': 23,\n",
       " '20220823': 22,\n",
       " '20220824': 36,\n",
       " '20220825': 22,\n",
       " '20220826': 47,\n",
       " '20220827': 24,\n",
       " '20220828': 13,\n",
       " '20220829': 30,\n",
       " '20220830': 36,\n",
       " '20220831': 46,\n",
       " '20220901': 30,\n",
       " '20220902': 47,\n",
       " '20220903': 57,\n",
       " '20220904': 10,\n",
       " '20220905': 14,\n",
       " '20220906': 23,\n",
       " '20220907': 46,\n",
       " '20220908': 47,\n",
       " '20220909': 46,\n",
       " '20220910': 61,\n",
       " '20220911': 61,\n",
       " '20220912': 71,\n",
       " '20220913': 66,\n",
       " '20220914': 32,\n",
       " '20220915': 23,\n",
       " '20220916': 26,\n",
       " '20220917': 17,\n",
       " '20220918': 12,\n",
       " '20220919': 24,\n",
       " '20220920': 28,\n",
       " '20220921': 31,\n",
       " '20220922': 86,\n",
       " '20220923': 20,\n",
       " '20220924': 26,\n",
       " '20220925': 24,\n",
       " '20220926': 28,\n",
       " '20220927': 44,\n",
       " '20220928': 45,\n",
       " '20220929': 16,\n",
       " '20220930': 23,\n",
       " '20221001': 22,\n",
       " '20221002': 25,\n",
       " '20221003': 32,\n",
       " '20221004': 39,\n",
       " '20221005': 52,\n",
       " '20221006': 40,\n",
       " '20221007': 41,\n",
       " '20221008': 44,\n",
       " '20221009': 38,\n",
       " '20221010': 56,\n",
       " '20221011': 21,\n",
       " '20221012': 39,\n",
       " '20221013': None,\n",
       " '20221017': None,\n",
       " '20221018': 45,\n",
       " '20221019': 54,\n",
       " '20221020': 60,\n",
       " '20221021': 31,\n",
       " '20221022': 5,\n",
       " '20221023': 5,\n",
       " '20221024': 8,\n",
       " '20221025': 6,\n",
       " '20221026': 7,\n",
       " '20221027': 12,\n",
       " '20221028': 11,\n",
       " '20221029': 7,\n",
       " '20221030': 8,\n",
       " '20221031': 6,\n",
       " '20221101': 2,\n",
       " '20221102': 5,\n",
       " '20221103': 6,\n",
       " '20221104': 31,\n",
       " '20221105': 9,\n",
       " '20221106': 4,\n",
       " '20221107': 3,\n",
       " '20221108': 9,\n",
       " '20221109': 15,\n",
       " '20221110': 12,\n",
       " '20221111': 13,\n",
       " '20221112': 14,\n",
       " '20221113': 18,\n",
       " '20221114': 18,\n",
       " '20221115': 14,\n",
       " '20221116': 10,\n",
       " '20221117': 17,\n",
       " '20221118': 17,\n",
       " '20221119': 19,\n",
       " '20221120': 19,\n",
       " '20221121': 26,\n",
       " '20221122': 27,\n",
       " '20221123': 20,\n",
       " '20221124': 10,\n",
       " '20221125': 8,\n",
       " '20221126': 5,\n",
       " '20221127': 5,\n",
       " '20221128': 6,\n",
       " '20221129': 6,\n",
       " '20221130': 6,\n",
       " '20221201': 9,\n",
       " '20221202': 9,\n",
       " '20221203': 6,\n",
       " '20221204': 6,\n",
       " '20221205': 6,\n",
       " '20221206': 5,\n",
       " '20221207': 6,\n",
       " '20221208': 6,\n",
       " '20221209': 7,\n",
       " '20221210': 8,\n",
       " '20221211': 6,\n",
       " '20221212': 7,\n",
       " '20221213': 12,\n",
       " '20221214': 10,\n",
       " '20221215': 8,\n",
       " '20221216': 10,\n",
       " '20221217': 10,\n",
       " '20221218': 6,\n",
       " '20221219': 5,\n",
       " '20221220': 4,\n",
       " '20221221': 7,\n",
       " '20221222': 4,\n",
       " '20221223': 7,\n",
       " '20221224': 5,\n",
       " '20221225': 6,\n",
       " '20221226': 9,\n",
       " '20221227': 8,\n",
       " '20221228': 2,\n",
       " '20221229': 6,\n",
       " '20221230': 6,\n",
       " '20221231': 4,\n",
       " '20230101': 4,\n",
       " '20230102': 2,\n",
       " '20230103': 5,\n",
       " '20230104': 6,\n",
       " '20230105': 6,\n",
       " '20230106': 13,\n",
       " '20230107': 6,\n",
       " '20230108': 7,\n",
       " '20230109': 6,\n",
       " '20230110': 6,\n",
       " '20230111': 11,\n",
       " '20230112': 6,\n",
       " '20230113': 8,\n",
       " '20230114': 7,\n",
       " '20230115': 5,\n",
       " '20230116': 2,\n",
       " '20230117': 7,\n",
       " '20230118': 8,\n",
       " '20230119': 4,\n",
       " '20230120': 10,\n",
       " '20230121': 9,\n",
       " '20230122': 14,\n",
       " '20230123': 14,\n",
       " '20230124': 18,\n",
       " '20230125': 20,\n",
       " '20230126': 13,\n",
       " '20230127': 6,\n",
       " '20230128': 6,\n",
       " '20230129': 4,\n",
       " '20230130': 11,\n",
       " '20230131': 13,\n",
       " '20230201': 13,\n",
       " '20230202': 15,\n",
       " '20230203': 21,\n",
       " '20230204': 12,\n",
       " '20230205': 4,\n",
       " '20230206': 4,\n",
       " '20230207': 3,\n",
       " '20230208': 6,\n",
       " '20230209': 8,\n",
       " '20230210': 13,\n",
       " '20230211': 15,\n",
       " '20230212': 12,\n",
       " '20230213': 16,\n",
       " '20230214': 6,\n",
       " '20230215': 10,\n",
       " '20230216': 15,\n",
       " '20230217': 10,\n",
       " '20230218': 5,\n",
       " '20230219': 36,\n",
       " '20230220': 116,\n",
       " '20230221': 37,\n",
       " '20230222': 45,\n",
       " '20230223': 55,\n",
       " '20230224': 7,\n",
       " '20230225': 11,\n",
       " '20230226': 33,\n",
       " '20230227': 7,\n",
       " '20230228': 6,\n",
       " '20230301': 6,\n",
       " '20230302': 35,\n",
       " '20230303': 12,\n",
       " '20230304': 10,\n",
       " '20230305': 6,\n",
       " '20230306': 6,\n",
       " '20230307': 10,\n",
       " '20230308': 6,\n",
       " '20230309': 8,\n",
       " '20230310': 2,\n",
       " '20230311': 5,\n",
       " '20230312': 4,\n",
       " '20230313': None,\n",
       " '20230314': None,\n",
       " '20230315': 5,\n",
       " '20230316': 8,\n",
       " '20230317': 10,\n",
       " '20230318': 11,\n",
       " '20230319': 12,\n",
       " '20230320': 9,\n",
       " '20230321': 11,\n",
       " '20230322': 14,\n",
       " '20230323': 24,\n",
       " '20230324': 6,\n",
       " '20230325': 2,\n",
       " '20230326': 3,\n",
       " '20230327': 8,\n",
       " '20230328': 12,\n",
       " '20230329': 6,\n",
       " '20230330': 11,\n",
       " '20230331': 6,\n",
       " '20230401': 13,\n",
       " '20230402': 2,\n",
       " '20230403': 2,\n",
       " '20230404': 3,\n",
       " '20230405': 6,\n",
       " '20230406': 9,\n",
       " '20230407': 6,\n",
       " '20230408': 4,\n",
       " '20230409': 8,\n",
       " '20230410': 4,\n",
       " '20230411': 5,\n",
       " '20230412': 6,\n",
       " '20230413': 5,\n",
       " '20230414': 12,\n",
       " '20230415': 10,\n",
       " '20230416': 12,\n",
       " '20230417': 7,\n",
       " '20230418': 9,\n",
       " '20230419': 7,\n",
       " '20230420': 7,\n",
       " '20230421': 6,\n",
       " '20230422': 10,\n",
       " '20230423': 10,\n",
       " '20230424': 6,\n",
       " '20230425': 8,\n",
       " '20230426': 17,\n",
       " '20230427': 15,\n",
       " '20230428': 28,\n",
       " '20230429': 30,\n",
       " '20230430': 37,\n",
       " '20230501': 20,\n",
       " '20230502': 19,\n",
       " '20230503': 31,\n",
       " '20230504': 60,\n",
       " '20230505': 4,\n",
       " '20230506': 4,\n",
       " '20230507': 6,\n",
       " '20230508': 8,\n",
       " '20230509': 11,\n",
       " '20230510': 14,\n",
       " '20230511': 15,\n",
       " '20230512': 25,\n",
       " '20230513': 26,\n",
       " '20230514': 13,\n",
       " '20230515': 17,\n",
       " '20230516': 19,\n",
       " '20230517': 13,\n",
       " '20230518': 34,\n",
       " '20230519': 53,\n",
       " '20230520': 62,\n",
       " '20230521': 50,\n",
       " '20230522': 23,\n",
       " '20230523': 13,\n",
       " '20230524': 18,\n",
       " '20230525': 19,\n",
       " '20230526': 14,\n",
       " '20230527': 22,\n",
       " '20230528': 14,\n",
       " '20230529': 13,\n",
       " '20230530': 22,\n",
       " '20230531': 25,\n",
       " '20230601': 17,\n",
       " '20230602': 18,\n",
       " '20230603': 17,\n",
       " '20230604': 15,\n",
       " '20230605': 19,\n",
       " '20230606': 27,\n",
       " '20230607': 20,\n",
       " '20230608': 38,\n",
       " '20230609': 13,\n",
       " '20230610': 6,\n",
       " '20230611': 6,\n",
       " '20230612': 15,\n",
       " '20230613': 56,\n",
       " '20230614': None,\n",
       " '20230621': None,\n",
       " '20230622': 19,\n",
       " '20230623': 21,\n",
       " '20230624': 13,\n",
       " '20230625': 13,\n",
       " '20230626': 19,\n",
       " '20230627': 25,\n",
       " '20230628': 27,\n",
       " '20230629': 19,\n",
       " '20230630': 20,\n",
       " '20230701': 19,\n",
       " '20230702': 20,\n",
       " '20230703': 21,\n",
       " '20230704': 25,\n",
       " '20230705': 35,\n",
       " '20230706': 28,\n",
       " '20230707': 32,\n",
       " '20230708': 29,\n",
       " '20230709': 26,\n",
       " '20230710': 31,\n",
       " '20230711': 16,\n",
       " '20230712': 18,\n",
       " '20230713': 14,\n",
       " '20230714': 23,\n",
       " '20230715': 23,\n",
       " '20230716': 18,\n",
       " '20230717': 35,\n",
       " '20230718': 15,\n",
       " '20230719': 29,\n",
       " '20230720': 28,\n",
       " '20230721': 39,\n",
       " '20230722': 37,\n",
       " '20230723': 23,\n",
       " '20230724': 58,\n",
       " '20230725': 17,\n",
       " '20230726': 18,\n",
       " '20230727': 29,\n",
       " '20230728': 23,\n",
       " '20230729': 21,\n",
       " '20230730': 17,\n",
       " '20230731': 24}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm10_aqi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "510d3c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pm10_aqi_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55547c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20220101': 40,\n",
       " '20220102': 34,\n",
       " '20220103': 31,\n",
       " '20220104': 38,\n",
       " '20220105': 36,\n",
       " '20220106': 23,\n",
       " '20220107': 8,\n",
       " '20220108': 12,\n",
       " '20220109': 21,\n",
       " '20220110': 33,\n",
       " '20220111': 44,\n",
       " '20220112': 48,\n",
       " '20220113': 57,\n",
       " '20220114': 50,\n",
       " '20220115': 31,\n",
       " '20220116': 24,\n",
       " '20220117': 52,\n",
       " '20220118': 31,\n",
       " '20220119': 37,\n",
       " '20220120': 30,\n",
       " '20220121': 10,\n",
       " '20220122': 24,\n",
       " '20220123': 26,\n",
       " '20220124': 36,\n",
       " '20220125': 46,\n",
       " '20220126': 31,\n",
       " '20220127': 25,\n",
       " '20220128': 41,\n",
       " '20220129': 58,\n",
       " '20220130': 56,\n",
       " '20220131': 4,\n",
       " '20220201': 8,\n",
       " '20220202': 15,\n",
       " '20220203': 24,\n",
       " '20220204': 38,\n",
       " '20220205': 17,\n",
       " '20220206': 44,\n",
       " '20220207': 28,\n",
       " '20220208': 15,\n",
       " '20220209': 28,\n",
       " '20220210': 23,\n",
       " '20220211': 15,\n",
       " '20220212': 18,\n",
       " '20220213': 33,\n",
       " '20220214': 27,\n",
       " '20220215': 7,\n",
       " '20220216': 10,\n",
       " '20220217': 13,\n",
       " '20220218': 17,\n",
       " '20220219': 13,\n",
       " '20220220': 3,\n",
       " '20220221': 10,\n",
       " '20220222': 14,\n",
       " '20220223': 15,\n",
       " '20220224': 21,\n",
       " '20220225': 35,\n",
       " '20220226': 37,\n",
       " '20220227': 43,\n",
       " '20220228': 20,\n",
       " '20220301': 5,\n",
       " '20220302': 10,\n",
       " '20220303': 4,\n",
       " '20220304': 6,\n",
       " '20220305': 9,\n",
       " '20220306': 11,\n",
       " '20220307': 13,\n",
       " '20220308': 6,\n",
       " '20220309': 7,\n",
       " '20220310': 9,\n",
       " '20220311': 20,\n",
       " '20220312': 16,\n",
       " '20220313': 3,\n",
       " '20220314': 9,\n",
       " '20220315': 6,\n",
       " '20220316': 5,\n",
       " '20220317': 10,\n",
       " '20220318': 6,\n",
       " '20220319': 8,\n",
       " '20220320': 4,\n",
       " '20220321': 7,\n",
       " '20220322': 11,\n",
       " '20220323': 10,\n",
       " '20220324': 6,\n",
       " '20220325': 13,\n",
       " '20220326': 16,\n",
       " '20220327': 17,\n",
       " '20220328': 16,\n",
       " '20220329': 7,\n",
       " '20220330': 7,\n",
       " '20220331': 4,\n",
       " '20220401': 8,\n",
       " '20220402': 7,\n",
       " '20220403': 10,\n",
       " '20220404': 10,\n",
       " '20220405': 7,\n",
       " '20220406': 11,\n",
       " '20220407': 13,\n",
       " '20220408': 10,\n",
       " '20220409': 5,\n",
       " '20220410': 3,\n",
       " '20220411': 4,\n",
       " '20220412': 8,\n",
       " '20220413': 10,\n",
       " '20220414': 15,\n",
       " '20220415': 11,\n",
       " '20220416': 6,\n",
       " '20220417': 4,\n",
       " '20220418': 6,\n",
       " '20220420': None,\n",
       " '20220421': 12,\n",
       " '20220422': 13,\n",
       " '20220423': 9,\n",
       " '20220424': 17,\n",
       " '20220425': 17,\n",
       " '20220426': 11,\n",
       " '20220427': 8,\n",
       " '20220428': 15,\n",
       " '20220429': 9,\n",
       " '20220430': 13,\n",
       " '20220501': 10,\n",
       " '20220502': 6,\n",
       " '20220503': 4,\n",
       " '20220504': 8,\n",
       " '20220505': 8,\n",
       " '20220506': 4,\n",
       " '20220507': 4,\n",
       " '20220508': 4,\n",
       " '20220509': 5,\n",
       " '20220510': 6,\n",
       " '20220511': 8,\n",
       " '20220512': 7,\n",
       " '20220513': 9,\n",
       " '20220514': 15,\n",
       " '20220515': 11,\n",
       " '20220516': 6,\n",
       " '20220517': 10,\n",
       " '20220518': 11,\n",
       " '20220519': 7,\n",
       " '20220520': 8,\n",
       " '20220521': 7,\n",
       " '20220522': 9,\n",
       " '20220523': 12,\n",
       " '20220524': 13,\n",
       " '20220525': 11,\n",
       " '20220526': 14,\n",
       " '20220527': 9,\n",
       " '20220528': 8,\n",
       " '20220529': 6,\n",
       " '20220530': 5,\n",
       " '20220531': 10,\n",
       " '20220601': 15,\n",
       " '20220602': 12,\n",
       " '20220603': 13,\n",
       " '20220604': 7,\n",
       " '20220605': 7,\n",
       " '20220606': 5,\n",
       " '20220607': 11,\n",
       " '20220608': 9,\n",
       " '20220609': 10,\n",
       " '20220610': 5,\n",
       " '20220611': 5,\n",
       " '20220612': 7,\n",
       " '20220613': 10,\n",
       " '20220614': 15,\n",
       " '20220615': 19,\n",
       " '20220616': 19,\n",
       " '20220617': 14,\n",
       " '20220618': 10,\n",
       " '20220619': 8,\n",
       " '20220620': 13,\n",
       " '20220621': 18,\n",
       " '20220622': 15,\n",
       " '20220623': 11,\n",
       " '20220624': 10,\n",
       " '20220625': 12,\n",
       " '20220626': 17,\n",
       " '20220627': 21,\n",
       " '20220628': 14,\n",
       " '20220629': 15,\n",
       " '20220630': 16,\n",
       " '20220701': 20,\n",
       " '20220702': 23,\n",
       " '20220703': 17,\n",
       " '20220704': 16,\n",
       " '20220705': 16,\n",
       " '20220706': 16,\n",
       " '20220707': 13,\n",
       " '20220708': 15,\n",
       " '20220709': 14,\n",
       " '20220710': 14,\n",
       " '20220711': 16,\n",
       " '20220712': 18,\n",
       " '20220713': 14,\n",
       " '20220714': 15,\n",
       " '20220715': 15,\n",
       " '20220716': 14,\n",
       " '20220717': 15,\n",
       " '20220718': 13,\n",
       " '20220719': 18,\n",
       " '20220720': 18,\n",
       " '20220721': 15,\n",
       " '20220722': 18,\n",
       " '20220723': 15,\n",
       " '20220724': 20,\n",
       " '20220725': 25,\n",
       " '20220726': 25,\n",
       " '20220727': 30,\n",
       " '20220728': 31,\n",
       " '20220729': 32,\n",
       " '20220730': 35,\n",
       " '20220731': 32,\n",
       " '20220801': 30,\n",
       " '20220802': 17,\n",
       " '20220803': 20,\n",
       " '20220804': 21,\n",
       " '20220805': 20,\n",
       " '20220806': 27,\n",
       " '20220807': 26,\n",
       " '20220808': 26,\n",
       " '20220809': 35,\n",
       " '20220810': 31,\n",
       " '20220811': 25,\n",
       " '20220812': 33,\n",
       " '20220813': 18,\n",
       " '20220814': 15,\n",
       " '20220815': 18,\n",
       " '20220816': 20,\n",
       " '20220817': 25,\n",
       " '20220818': 31,\n",
       " '20220819': 38,\n",
       " '20220820': 30,\n",
       " '20220821': 13,\n",
       " '20220822': 18,\n",
       " '20220823': 22,\n",
       " '20220824': 28,\n",
       " '20220825': 26,\n",
       " '20220826': 24,\n",
       " '20220827': 12,\n",
       " '20220828': 17,\n",
       " '20220829': 24,\n",
       " '20220830': 26,\n",
       " '20220831': 30,\n",
       " '20220901': 49,\n",
       " '20220902': 57,\n",
       " '20220903': 35,\n",
       " '20220904': 12,\n",
       " '20220905': 14,\n",
       " '20220906': 16,\n",
       " '20220907': 19,\n",
       " '20220908': 53,\n",
       " '20220909': 47,\n",
       " '20220910': 87,\n",
       " '20220911': 105,\n",
       " '20220912': 151,\n",
       " '20220913': 142,\n",
       " '20220914': 44,\n",
       " '20220915': 34,\n",
       " '20220916': 20,\n",
       " '20220917': 15,\n",
       " '20220918': 18,\n",
       " '20220919': 30,\n",
       " '20220920': 16,\n",
       " '20220921': 18,\n",
       " '20220922': 26,\n",
       " '20220923': 19,\n",
       " '20220924': 28,\n",
       " '20220925': 25,\n",
       " '20220926': 28,\n",
       " '20220927': 38,\n",
       " '20220928': 29,\n",
       " '20220929': 16,\n",
       " '20220930': 42,\n",
       " '20221001': 43,\n",
       " '20221002': 41,\n",
       " '20221003': 35,\n",
       " '20221004': 52,\n",
       " '20221005': 62,\n",
       " '20221006': 60,\n",
       " '20221007': 59,\n",
       " '20221008': 61,\n",
       " '20221009': 62,\n",
       " '20221010': 55,\n",
       " '20221011': 23,\n",
       " '20221012': 48,\n",
       " '20221013': 45,\n",
       " '20221014': 52,\n",
       " '20221015': 53,\n",
       " '20221016': 43,\n",
       " '20221017': 45,\n",
       " '20221018': 57,\n",
       " '20221019': 67,\n",
       " '20221020': 70,\n",
       " '20221021': 29,\n",
       " '20221022': 10,\n",
       " '20221023': 13,\n",
       " '20221024': 8,\n",
       " '20221025': 5,\n",
       " '20221026': 7,\n",
       " '20221027': 15,\n",
       " '20221028': 13,\n",
       " '20221029': 14,\n",
       " '20221030': 12,\n",
       " '20221031': 6,\n",
       " '20221101': 7,\n",
       " '20221102': 9,\n",
       " '20221103': 19,\n",
       " '20221104': 9,\n",
       " '20221105': 7,\n",
       " '20221106': 8,\n",
       " '20221107': 6,\n",
       " '20221108': 19,\n",
       " '20221109': 28,\n",
       " '20221110': 31,\n",
       " '20221111': 38,\n",
       " '20221112': 39,\n",
       " '20221113': 46,\n",
       " '20221114': 44,\n",
       " '20221115': 43,\n",
       " '20221116': 35,\n",
       " '20221117': 42,\n",
       " '20221118': 38,\n",
       " '20221119': 41,\n",
       " '20221120': 44,\n",
       " '20221121': 55,\n",
       " '20221122': 55,\n",
       " '20221123': 50,\n",
       " '20221124': 34,\n",
       " '20221125': 24,\n",
       " '20221126': 15,\n",
       " '20221127': 9,\n",
       " '20221128': 12,\n",
       " '20221129': 18,\n",
       " '20221130': 20,\n",
       " '20221201': 33,\n",
       " '20221202': 21,\n",
       " '20221203': 20,\n",
       " '20221204': 17,\n",
       " '20221205': 22,\n",
       " '20221206': 19,\n",
       " '20221207': 20,\n",
       " '20221208': 21,\n",
       " '20221209': 25,\n",
       " '20221210': 22,\n",
       " '20221211': 17,\n",
       " '20221212': 18,\n",
       " '20221213': 25,\n",
       " '20221214': 23,\n",
       " '20221215': 38,\n",
       " '20221216': 48,\n",
       " '20221217': 53,\n",
       " '20221218': 36,\n",
       " '20221219': 14,\n",
       " '20221220': 20,\n",
       " '20221221': 17,\n",
       " '20221222': 13,\n",
       " '20221223': 14,\n",
       " '20221224': 24,\n",
       " '20221225': 31,\n",
       " '20221226': 49,\n",
       " '20221227': 15,\n",
       " '20221228': 3,\n",
       " '20221229': 11,\n",
       " '20221230': 10,\n",
       " '20221231': 8,\n",
       " '20230101': 11,\n",
       " '20230102': 9,\n",
       " '20230103': 11,\n",
       " '20230104': 13,\n",
       " '20230105': 11,\n",
       " '20230106': 20,\n",
       " '20230107': 20,\n",
       " '20230108': 17,\n",
       " '20230109': 14,\n",
       " '20230110': 15,\n",
       " '20230111': 23,\n",
       " '20230112': 17,\n",
       " '20230113': 21,\n",
       " '20230114': 18,\n",
       " '20230115': 9,\n",
       " '20230116': 4,\n",
       " '20230117': 13,\n",
       " '20230118': 20,\n",
       " '20230119': 5,\n",
       " '20230120': 13,\n",
       " '20230121': 18,\n",
       " '20230122': 28,\n",
       " '20230123': 20,\n",
       " '20230124': 25,\n",
       " '20230125': 30,\n",
       " '20230126': 23,\n",
       " '20230127': 6,\n",
       " '20230128': 5,\n",
       " '20230129': 13,\n",
       " '20230130': 25,\n",
       " '20230131': 26,\n",
       " '20230201': 30,\n",
       " '20230202': 40,\n",
       " '20230203': 45,\n",
       " '20230204': 25,\n",
       " '20230205': 9,\n",
       " '20230206': 5,\n",
       " '20230207': 5,\n",
       " '20230208': 8,\n",
       " '20230209': 13,\n",
       " '20230210': 18,\n",
       " '20230211': 22,\n",
       " '20230212': 20,\n",
       " '20230213': 6,\n",
       " '20230214': 7,\n",
       " '20230215': 10,\n",
       " '20230216': 15,\n",
       " '20230217': 9,\n",
       " '20230218': 6,\n",
       " '20230219': 9,\n",
       " '20230220': 13,\n",
       " '20230221': 5,\n",
       " '20230222': 13,\n",
       " '20230223': 16,\n",
       " '20230224': 18,\n",
       " '20230225': 31,\n",
       " '20230226': 19,\n",
       " '20230227': 10,\n",
       " '20230228': 8,\n",
       " '20230301': 8,\n",
       " '20230302': 8,\n",
       " '20230303': 8,\n",
       " '20230304': 20,\n",
       " '20230305': 17,\n",
       " '20230306': 7,\n",
       " '20230307': 11,\n",
       " '20230308': 9,\n",
       " '20230309': 7,\n",
       " '20230310': 3,\n",
       " '20230311': 9,\n",
       " '20230312': 9,\n",
       " '20230313': 9,\n",
       " '20230314': 5,\n",
       " '20230315': 6,\n",
       " '20230316': 11,\n",
       " '20230317': 13,\n",
       " '20230318': 17,\n",
       " '20230319': 19,\n",
       " '20230320': 18,\n",
       " '20230321': 18,\n",
       " '20230322': 19,\n",
       " '20230323': 7,\n",
       " '20230324': 3,\n",
       " '20230325': 5,\n",
       " '20230326': 5,\n",
       " '20230327': 7,\n",
       " '20230328': 7,\n",
       " '20230329': 9,\n",
       " '20230330': 12,\n",
       " '20230331': 8,\n",
       " '20230401': 4,\n",
       " '20230402': 3,\n",
       " '20230403': 3,\n",
       " '20230404': 4,\n",
       " '20230405': 8,\n",
       " '20230406': 11,\n",
       " '20230407': 8,\n",
       " '20230408': 7,\n",
       " '20230409': 14,\n",
       " '20230410': 7,\n",
       " '20230411': 4,\n",
       " '20230412': 9,\n",
       " '20230413': 7,\n",
       " '20230414': 11,\n",
       " '20230415': 13,\n",
       " '20230416': 11,\n",
       " '20230417': 12,\n",
       " '20230418': 6,\n",
       " '20230419': 6,\n",
       " '20230420': 9,\n",
       " '20230421': 10,\n",
       " '20230422': 15,\n",
       " '20230423': 11,\n",
       " '20230424': 4,\n",
       " '20230425': 6,\n",
       " '20230426': 11,\n",
       " '20230427': 11,\n",
       " '20230428': 17,\n",
       " '20230429': 21,\n",
       " '20230430': 14,\n",
       " '20230501': 12,\n",
       " '20230502': 14,\n",
       " '20230503': 23,\n",
       " '20230504': 15,\n",
       " '20230505': 4,\n",
       " '20230506': 5,\n",
       " '20230507': 7,\n",
       " '20230508': 10,\n",
       " '20230509': 10,\n",
       " '20230510': 14,\n",
       " '20230511': 15,\n",
       " '20230512': 17,\n",
       " '20230513': 20,\n",
       " '20230514': 12,\n",
       " '20230515': 13,\n",
       " '20230516': 17,\n",
       " '20230517': 20,\n",
       " '20230518': 57,\n",
       " '20230519': 70,\n",
       " '20230520': 34,\n",
       " '20230521': 45,\n",
       " '20230522': 16,\n",
       " '20230523': 18,\n",
       " '20230524': 19,\n",
       " '20230525': 26,\n",
       " '20230526': 19,\n",
       " '20230527': 26,\n",
       " '20230528': 21,\n",
       " '20230529': 20,\n",
       " '20230530': 18,\n",
       " '20230531': 19,\n",
       " '20230601': 20,\n",
       " '20230602': 22,\n",
       " '20230603': 22,\n",
       " '20230604': 21,\n",
       " '20230605': 19,\n",
       " '20230606': 21,\n",
       " '20230607': 21,\n",
       " '20230608': 33,\n",
       " '20230609': 28,\n",
       " '20230610': 17,\n",
       " '20230611': 13,\n",
       " '20230612': 24,\n",
       " '20230613': 33,\n",
       " '20230614': 17,\n",
       " '20230615': 35,\n",
       " '20230616': 42,\n",
       " '20230617': 8,\n",
       " '20230618': 8,\n",
       " '20230619': 7,\n",
       " '20230620': 9,\n",
       " '20230621': 10,\n",
       " '20230622': 18,\n",
       " '20230623': 22,\n",
       " '20230624': 22,\n",
       " '20230625': 24,\n",
       " '20230626': 29,\n",
       " '20230627': 25,\n",
       " '20230628': 34,\n",
       " '20230629': 24,\n",
       " '20230630': 18,\n",
       " '20230701': 19,\n",
       " '20230702': 18,\n",
       " '20230703': 19,\n",
       " '20230704': 47,\n",
       " '20230705': 52,\n",
       " '20230706': 36,\n",
       " '20230707': 37,\n",
       " '20230708': 33,\n",
       " '20230709': 26,\n",
       " '20230710': 18,\n",
       " '20230711': 18,\n",
       " '20230712': 20,\n",
       " '20230713': 16,\n",
       " '20230714': 21,\n",
       " '20230715': 20,\n",
       " '20230716': 18,\n",
       " '20230717': 13,\n",
       " '20230718': 14,\n",
       " '20230719': 20,\n",
       " '20230720': 28,\n",
       " '20230721': 35,\n",
       " '20230722': 59,\n",
       " '20230723': 32,\n",
       " '20230724': 23,\n",
       " '20230725': 15,\n",
       " '20230726': 21,\n",
       " '20230727': 23,\n",
       " '20230728': 21,\n",
       " '20230729': 26,\n",
       " '20230730': 23,\n",
       " '20230731': 28}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm25_aqi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d921746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pm25_aqi_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217ebb2",
   "metadata": {},
   "source": [
    "Now, my classmate, Emily Rolen, recommended I select the larger AQI value of pm10 and pm2.5 for my overall AQI value for each day, and I like this idea. Based upon my exploratory work earlier into the 0002 Site sensor, we know that the pm10 detector has an open date of 2019-04-01, while the pm2.5 acceptable has an open date of 2005-10-19. Based upon this info, let's find the max AQI and store it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c88cac",
   "metadata": {},
   "source": [
    "**TBD Decision, right now, the next cell allows AQI to be None if necessary. should I change to a constant like '-1'?**\n",
    "**Also, issue of missing keys from pm10!!!??????????????**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab380ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 576/576 [00:00<00:00, 2068423.89it/s]\n"
     ]
    }
   ],
   "source": [
    "pm10_aqi_keys = pm10_aqi_dict.keys()\n",
    "pm25_aqi_keys = pm25_aqi_dict.keys()\n",
    "\n",
    "aqi_dict = {}\n",
    "pm25_key_index = 0\n",
    "# iterate through the pm2.5 data. we can assume the pm2.5 aqi dataframe has more values since it's monitor has been open longer\n",
    "for key in tqdm(pm25_aqi_keys):\n",
    "    pm25_aqi = pm25_aqi_dict[key]\n",
    "    \n",
    "    #check if we have reached first possible date for both dataframes to have values\n",
    "    try:\n",
    "        # check if the pm10 dictionary has an entry for the current date/key\n",
    "        pm10_aqi = pm10_aqi_dict[key]\n",
    "    \n",
    "    except: #if key doesn't exist use the value from pm25\n",
    "        max_aqi = pm25_aqi\n",
    "    \n",
    "    else: #if pm10 does have an entry for that key choose the max\n",
    "        \n",
    "        # handle null values\n",
    "        if (pm10_aqi is None) and (pm25_aqi is None):\n",
    "            max_aqi = None\n",
    "            \n",
    "        elif pm10_aqi is None:\n",
    "            max_aqi = pm25_aqi\n",
    "            \n",
    "        elif pm25_aqi is None:\n",
    "            max_aqi = pm10_aqi\n",
    "            \n",
    "        else: # if neither are null compare them and pick largest\n",
    "            if pm10_aqi >= pm25_aqi:\n",
    "                max_aqi = pm10_aqi\n",
    "            else:\n",
    "                max_aqi = pm25_aqi \n",
    "        \n",
    "    #save the value\n",
    "    aqi_dict[key] = max_aqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e049ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqi_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a5632",
   "metadata": {},
   "source": [
    "**TBD: understand why comment doesn't work**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea99eb2",
   "metadata": {},
   "source": [
    "Now we convert the AQI dictionary to a pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "847ec8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20220101</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220102</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220103</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220104</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20220105</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>20230727</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>20230728</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>20230729</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>20230730</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>20230731</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AQI\n",
       "0    20220101   40\n",
       "1    20220102   34\n",
       "2    20220103   31\n",
       "3    20220104   38\n",
       "4    20220105   36\n",
       "..        ...  ...\n",
       "571  20230727   29\n",
       "572  20230728   23\n",
       "573  20230729   26\n",
       "574  20230730   23\n",
       "575  20230731   28\n",
       "\n",
       "[576 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I referenced: https://www.geeksforgeeks.org/how-to-create-dataframe-from-dictionary-in-python-pandas/#\n",
    "# and: https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi\n",
    "# and: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html\n",
    "# and: https://sparkbyexamples.com/pandas/pandas-create-new-dataframe-by-selecting-specific-columns/#:~:text=You%20can%20create%20a%20new,added%20to%20the%20original%20ones.\n",
    "# df_aqi = pd.DataFrame.from_records(aqi_dict)\n",
    "# helpful to understand erorr in line above: df_aqi = pd.DataFrame().assign(aqi_dict)\n",
    "\n",
    "df_daily_aqi = pd.DataFrame({\n",
    "    'Date' : aqi_dict.keys(),\n",
    "    'AQI' : aqi_dict.values()\n",
    "})\n",
    "df_daily_aqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915c1ad",
   "metadata": {},
   "source": [
    "**TBD:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3108dfd",
   "metadata": {},
   "source": [
    "Potential Issues/Big Assumptions:\n",
    "- This method will ensure that every date in the pm2.5 data will be included, but no check for whether every date in pm10 will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88950704",
   "metadata": {},
   "source": [
    "Next, we find the annual AQI. First we have to add a column to the daily aqi data frame representing each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9095df35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Date</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>20220101</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>20220102</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>20220103</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>20220104</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>20220105</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>2023</td>\n",
       "      <td>20230727</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>2023</td>\n",
       "      <td>20230728</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>2023</td>\n",
       "      <td>20230729</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>2023</td>\n",
       "      <td>20230730</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>2023</td>\n",
       "      <td>20230731</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year      Date  AQI\n",
       "0    2022  20220101   40\n",
       "1    2022  20220102   34\n",
       "2    2022  20220103   31\n",
       "3    2022  20220104   38\n",
       "4    2022  20220105   36\n",
       "..    ...       ...  ...\n",
       "571  2023  20230727   29\n",
       "572  2023  20230728   23\n",
       "573  2023  20230729   26\n",
       "574  2023  20230730   23\n",
       "575  2023  20230731   28\n",
       "\n",
       "[576 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sources I referenced to review how to format dates:\n",
    "# https://docs.python.org/3/library/datetime.html#format-codes\n",
    "# https://stackoverflow.com/questions/55542280/why-does-python-3-find-this-iso8601-date-2019-04-05t165526z-invalid\n",
    "# https://stackoverflow.com/questions/19934248/nameerror-name-datetime-is-not-defined\n",
    "\n",
    "my_date = datetime.strptime('20210701', '%Y%m%d')\n",
    "my_date.year\n",
    "\n",
    "year_list = []\n",
    "# iterate through the dataframe\n",
    "for index, row in df_daily_aqi.iterrows():\n",
    "    date = row['Date']\n",
    "    my_date = datetime.strptime(date, '%Y%m%d')\n",
    "    year_list.append(my_date.year)\n",
    "    \n",
    "# add the years to the dataframe\n",
    "df_daily_aqi['Year'] = year_list\n",
    "df_daily_aqi = df_daily_aqi[['Year', 'Date', 'AQI']]\n",
    "df_daily_aqi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0551b9a8",
   "metadata": {},
   "source": [
    "Note, that there may be days when the sensor did not record data for either pollutant type. For example, I tested the code on acquiring data from 2021 and the dataframe with daily AQI values had data for 343 rows, not 365. While this is not ideal, we proceed using this method as it still provides some helpful data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241eb9e6",
   "metadata": {},
   "source": [
    "**LO: If have time add a column daily aqi dataframe for tracking what proportion of days each year had data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b192fe6",
   "metadata": {},
   "source": [
    "Now that we have the daily aqi, let's find the average aqi for the recorded days for each year and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc68815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_654/2730940416.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_annual_avg_aqi = df_daily_aqi.groupby('Year').agg('mean')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>24.587912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>20.179245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AQI\n",
       "Year           \n",
       "2022  24.587912\n",
       "2023  20.179245"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I viewed: https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\n",
    "# to remind myself how to do a groupby\n",
    "df_annual_avg_aqi = df_daily_aqi.groupby('Year').agg('mean')\n",
    "df_annual_avg_aqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f085a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annual_avg_aqi.to_csv('Annual_AQI_Average.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29f690",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- create a dataframe\n",
    "- add a column for smoke estimate calculation\n",
    "\n",
    "For the step 2 visualizations, the data should be in a dataframe form. That way I can group by year\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b143fc4",
   "metadata": {},
   "source": [
    "## References:\n",
    "- Learning to use .gitignore:\n",
    "    * https://stackoverflow.com/questions/19663093/apply-gitignore-on-an-existing-repository-already-tracking-large-number-of-file\n",
    "    * https://stackoverflow.com/questions/40441450/relative-parent-directory-path-for-gitignore\n",
    "- Learning to work with GIS data:\n",
    "    * [3] wildfire_geo_proximity_example.ipynb. //\"This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.0 - August 13, 2023\"\n",
    "- [4]. Proffessor's wildfire module\n",
    "- [5]. https://www.latlong.net/place/west-richland-wa-usa-25340.html\n",
    "- [6]. https://stackoverflow.com/questions/12268930/create-json-with-multiple-dictionaries-python\n",
    "- [7]. https://www.geeksforgeeks.org/reading-and-writing-json-to-a-file-in-python/#\n",
    "- [8]. https://www.w3schools.com/python/python_try_except.asp\n",
    "- [9]. 'DATA 512 Homework 1 Acquire Process and Analyze Data.ipynb' at: https://github.com/logan-obrien/data-512-homework_1.git\n",
    "- [10]. https://www.airnow.gov/aqi/aqi-basics/\n",
    "- [11]. https://www.google.com/search?q=how+to+convert+acres+to+square+miles&rlz=1C1RXQR_enUS1019US1019&oq=how+to+convert+from+acres+to+s&gs_lcrp=EgZjaHJvbWUqCAgDEAAYFhgeMgYIABBFGDkyCAgBEAAYFhgeMggIAhAAGBYYHjIICAMQABgWGB4yCAgEEAAYFhgeMgoIBRAAGIYDGIoFMgoIBhAAGIYDGIoF0gEIOTU3MmowajeoAgCwAgA&sourceid=chrome&ie=UTF-8\n",
    "- [12]. https://www.geeksforgeeks.org/numpy-argmax-python/\n",
    "- [13]. https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\n",
    "- [14]. epa_air_quality_history_example.ipynb //\"This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.1 - September 5, 2023\"\n",
    "- [15]. https://www.census.gov/library/reference/code-lists/ansi.html#cousub \n",
    "- [16]. https://www2.census.gov/geo/docs/reference/codes2020/cousub/st53_wa_cousub2020.txt\n",
    "- [17]. https://www.airnow.gov/sites/default/files/2020-05/aqi-technical-assistance-document-sept2018.pdf\n",
    "- [18]. https://www.epa.gov/wildfire-smoke-course/why-wildfire-smoke-health-concern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
